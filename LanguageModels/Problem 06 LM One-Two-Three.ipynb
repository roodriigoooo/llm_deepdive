{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Language Models for One-Two-Three Languages\n",
    "\n",
    "In this exercise we'll estimate ngram language models on two synthetic languages. For each language we'll receive sentences and estimate an LM for it, but in this excercise the languages will be represented by a single sentence!\n",
    "\n",
    "The first language is called L123 and is represented by this sentence:\n",
    "\n",
    "```\n",
    "   one two three\n",
    "```\n",
    "\n",
    "\n",
    "The second language is L1231 and is represented by this sentence:\n",
    "\n",
    "```\n",
    "   one two three one\n",
    "```\n",
    "\n",
    "Follow the sections of the exercise and answer the questions. The file `ngram_lm.py` has methods to estimate ngram language models, it must be placed in the same folder as this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:04:02.135095Z",
     "start_time": "2025-04-16T14:04:02.126095Z"
    }
   },
   "source": [
    "# Place the file \"ngram_lm.py\" to the same folder as this notebook\n",
    "from ngram_lm import count_ngrams_up_to, NGramLanguageModel, prob_text, text_generator\n",
    "\n",
    "# this is the data for the two languages in this exercise\n",
    "texts_123 = [\"one two three\"]\n",
    "texts_1231 = [\"one two three one\"]"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bigram model of L123\n",
    "\n",
    "We start estimating a bigram language model for L123. We first take counts up to `n_max=3` of the data (a single sentence), because later we'll use the same counts for trigam model. With the counts we \n",
    "create a bigram `NGramLanguageModel` instance, setting `n=2`. The last parameter controls smoothing, we set it at 0 for now. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:04:03.393684Z",
     "start_time": "2025-04-16T14:04:03.378501Z"
    }
   },
   "source": [
    "counts_123 = count_ngrams_up_to(n_max=3, texts=texts_123)\n",
    "bigram_lm_123 = NGramLanguageModel(n=2, ngram_counts=counts_123, back_off_discount=0)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:04:04.078410Z",
     "start_time": "2025-04-16T14:04:04.068410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "counts_1231 = count_ngrams_up_to(n_max=2, texts=texts_1231)\n",
    "print(counts_1231)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(): Counter({'one': 2, 'two': 1, 'three': 1, '_STOP_': 1}), ('_START_',): Counter({'one': 1}), ('one',): Counter({'two': 1, '_STOP_': 1}), ('two',): Counter({'three': 1}), ('three',): Counter({'one': 1})}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the probability of a sentence. Let's try the sentence of L123:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:04:05.567042Z",
     "start_time": "2025-04-16T14:04:05.553859Z"
    }
   },
   "source": [
    "prob_text(bigram_lm_123, \"one two three\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ngram language model computes the probability of a sentence by a product of n-gram probabilities. In a bigram model there is a probability term for each bigram of the language: given a word, what is the probability of a next word?  The bigram model estimates the distribution of next words for each word of the language, by taking counts from the training texts. \n",
    "\n",
    "Since a probability of a text is the probability of its bigrams, if a new text has an unseen bigram then the full probability will be 0. The model will give non-zero probability to all texts made with bigrams observed in the training texts: to create a text, choose a starting bigram, and keep tiling bigrams until generating a special stop symbol. This is a very rough behavior. Later we add smoothing, which combines ngram models with different n. \n",
    "\n",
    "Here we show the distributions of next words for the 4 possible contexts in the model of L123. Note that we include the the special symbol `_START_` as context, and the special `_STOP_` symbol as next word. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:04:06.715429Z",
     "start_time": "2025-04-16T14:04:06.705428Z"
    }
   },
   "source": [
    "bigram_lm_123.p_next_word([\"_START_\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 1.0, 'two': 0.0, 'three': 0.0, '_STOP_': 0.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:04:07.681582Z",
     "start_time": "2025-04-16T14:04:07.659586Z"
    }
   },
   "source": [
    "bigram_lm_123.p_next_word([\"one\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'two': 1.0, 'three': 0.0, '_STOP_': 0.0, 'one': 0.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:04:08.519552Z",
     "start_time": "2025-04-16T14:04:08.508352Z"
    }
   },
   "source": [
    "bigram_lm_123.p_next_word([\"two\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'three': 1.0, 'two': 0.0, '_STOP_': 0.0, 'one': 0.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:04:09.124354Z",
     "start_time": "2025-04-16T14:04:09.113352Z"
    }
   },
   "source": [
    "bigram_lm_123.p_next_word([\"three\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_STOP_': 1.0, 'two': 0.0, 'three': 0.0, 'one': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: A bigram model for L123. \n",
    "Compute the probability of these sentences using the bigram LM for L123:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:04:10.225830Z",
     "start_time": "2025-04-16T14:04:10.214831Z"
    }
   },
   "source": [
    "test_sentences = [\n",
    "    \"one two three\",\n",
    "    \"one two three one\",\n",
    "    \"one\", \n",
    "    \"two\",\n",
    "    \"three\",\n",
    "    \"one two\",\n",
    "    \"one three\"\n",
    "    \"one two three one two three\", \n",
    "    \"one two three one two three one\",\n",
    "    \"one two three one two three one two three one\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sentences can you find that have non-zero probability? Can you tell why? What is the probability sum over all possible sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: A bigram model for L1231. \n",
    "Estimate a bigram language model using the the text of L1231, without smoothing. Then compute the probability for the test sentences above and list the non-zero ones. \n",
    "\n",
    "Reason about how many sentences with non-zero probability can be generated with this bigram LM for L1231. Could you tell the sum of all these probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: A trigram model for L1231.\n",
    "Estimate a trigram model for L1231 and compute the probability of the test sentences. How many sentences can receive a non-zero probability using this trigram model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding smoothing for L123 \n",
    "\n",
    "We will now add \"smoothing\" to the counts, such that the ngram language models can assign non-zero probabilities to unseen combinations. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:06:05.886598Z",
     "start_time": "2025-04-16T14:06:05.861598Z"
    }
   },
   "source": [
    "bigram_lm_123_smoothed = NGramLanguageModel(2, counts_123, back_off_discount=0.1)\n",
    "for sentence in test_sentences:\n",
    "    p = prob_text(bigram_lm_123_smoothed, sentence)\n",
    "    print(sentence, p)\n",
    "\n",
    "print(bigram_lm_123_smoothed.p_next_word([\"one\"]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one two three 0.6561000000000001\n",
      "one two three one 0.0006561\n",
      "one 0.027\n",
      "two 0.0009\n",
      "three 0.027\n",
      "one two 0.024300000000000002\n",
      "one threeone two three one two three 0.0\n",
      "one two three one two three one 1.5943230000000002e-05\n",
      "one two three one two three one two three one 3.8742048900000004e-07\n",
      "{'two': 0.9, '_STOP_': 0.03, 'three': 0.03, 'one': 0.03}\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding this type of smoothing, a portion of counts is _discounted_ from the counts of observed ngrams and passed to unobserved ngrams. This implies that while before the probability of sentence `one two three` was 1, now it is lower because part of the mass has been passed to other sentences. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:14:31.633788Z",
     "start_time": "2025-04-16T14:14:31.609787Z"
    }
   },
   "source": [
    "prob_text(bigram_lm_123_smoothed, \"one two three\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6561000000000001"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look inside, the probabilities of the next word have been smoothed: the single count of 1 observed for bigram `one two` has been distributed to `one one`, `one three` and `one _STOP_` by even parts of the discount. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:14:32.997463Z",
     "start_time": "2025-04-16T14:14:32.990462Z"
    }
   },
   "source": [
    "bigram_lm_123_smoothed.p_next_word(['one'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'two': 0.9, '_STOP_': 0.03, 'three': 0.03, 'one': 0.03}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Smoothed bigram LM for L123. \n",
    "Using the smoothed LM for L123, compute the probability of the test sentences. Reason about the number of sentences with non-zero probability under this LM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Random sentences. \n",
    "The method `text_generator` generates a sentence using a language model, by taking the most likely next word until a `_STOP_` word is generated. It returns the probability of the generated sentence and the list of tokens of the sentence. \n",
    "\n",
    "With `randomize=True`, the choice of next word is random and proportional to the model distribution. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:18:45.696618Z",
     "start_time": "2025-04-16T14:18:45.686619Z"
    }
   },
   "source": [
    "lm = NGramLanguageModel(n=3, ngram_counts=counts_123, back_off_discount=0.1) \n",
    "for i in range(10):\n",
    "    print(text_generator(ngram_model=lm, randomize=True))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.024300000000000002, ['one', 'two', '_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.03, ['_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.0004782969, ['one', 'two', 'one', 'two', 'one', 'two', 'three', '_STOP_'])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different values of the back-off discount parameter, randomly sample sentences and see the variety you obtain. Try values of 0, 0.1, 0.3 and 0.5 and report your observations. Note that each sample is independent, and the sampler will keep repeating the most likely sentences."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:20:21.434156Z",
     "start_time": "2025-04-16T14:20:21.421155Z"
    }
   },
   "source": [
    "for discount in [0, 0.1, 0.3, 0.5]:\n",
    "    lm = NGramLanguageModel(n=3, ngram_counts=counts_123, back_off_discount=discount)\n",
    "    print(f\"discount={discount}\")\n",
    "    for i in range(10):\n",
    "        print(text_generator(ngram_model=lm, randomize=True))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discount=0\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "(1.0, ['one', 'two', 'three', '_STOP_'])\n",
      "discount=0.1\n",
      "(0.03, ['_STOP_'])\n",
      "(0.000531441, ['one', 'two', 'two', 'one', 'two', 'three', '_STOP_'])\n",
      "(0.03, ['_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.019683000000000003, ['one', 'two', 'three', 'three', '_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.015943230000000003, ['one', 'two', 'three', 'one', 'two', 'three', '_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.6561000000000001, ['one', 'two', 'three', '_STOP_'])\n",
      "discount=0.3\n",
      "(0.24009999999999992, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.0343, ['two', 'three', '_STOP_'])\n",
      "(0.048999999999999995, ['one', '_STOP_'])\n",
      "(0.06999999999999999, ['_STOP_'])\n",
      "(0.016806999999999996, ['one', 'two', 'three', 'three', '_STOP_'])\n",
      "(0.03429999999999999, ['one', 'two', '_STOP_'])\n",
      "(0.0343, ['one', 'three', '_STOP_'])\n",
      "(0.048999999999999995, ['one', '_STOP_'])\n",
      "(0.048999999999999995, ['three', '_STOP_'])\n",
      "(0.0011764899999999997, ['one', 'one', 'one', 'two', 'three', '_STOP_'])\n",
      "discount=0.5\n",
      "(0.00043402777777777775, ['one', 'two', 'two', 'three', 'three', '_STOP_'])\n",
      "(0.0008680555555555555, ['two', 'three', 'two', 'three', '_STOP_'])\n",
      "(0.0625, ['one', 'two', 'three', '_STOP_'])\n",
      "(0.020833333333333332, ['one', 'two', '_STOP_'])\n",
      "(0.041666666666666664, ['one', '_STOP_'])\n",
      "(7.233796296296296e-05, ['one', 'two', 'three', 'one', 'one', '_STOP_'])\n",
      "(1.808449074074074e-05, ['one', 'two', 'three', 'three', 'three', 'two', 'three', '_STOP_'])\n",
      "(0.020833333333333332, ['one', 'two', '_STOP_'])\n",
      "(0.003472222222222222, ['one', 'one', '_STOP_'])\n",
      "(0.00014467592592592592, ['one', 'two', 'one', 'one', '_STOP_'])\n"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esade-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
