{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-22T12:07:46.444762Z",
     "start_time": "2025-06-22T12:07:46.440435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os, sys\n",
    "import tiktoken\n",
    "\n",
    "project_root = os.path.abspath(\n",
    "    os.path.join(os.getcwd(), os.pardir, os.pardir)\n",
    ")\n",
    "\n",
    "stage1_root = os.path.join(project_root, \"stage1\")\n",
    "sys.path.insert(0, stage1_root)\n",
    "\n",
    "# now 'src' is a top-level package\n",
    "from src.gpt2small import GPTModel, GPTConfig124, generate_text\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T12:07:47.813300Z",
     "start_time": "2025-06-22T12:07:46.467785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg_pt = GPTConfig124(vocab_size=50257, context_length=256, emb_dim=768,\n",
    "                   n_heads=12, n_layers=12, dropout=0.1, qkv_bias=False)\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(cfg_pt)\n",
    "model.eval()"
   ],
   "id": "6aee549c14b1461",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_form): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T12:07:48.280417Z",
     "start_time": "2025-06-22T12:07:47.833892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_to_token_ids(text, tokenizer, allowed_special):\n",
    "    \"\"\"\n",
    "    tensor.unsqueeze(dim) inserts a new axis (of size 1) at index dim.\n",
    "    x = torch.tensor([10, 20, 30])       shape: [3]\n",
    "    x0 = x.unsqueeze(0)                 shape: [1,3]\n",
    "    x1 = x.unsqueeze(1)                 shape: [3,1]\n",
    "    \"\"\"\n",
    "    allowed_special = allowed_special or ('<|endoftext|>')\n",
    "    token_list = tokenizer.encode(text, allowed_special=set(allowed_special))\n",
    "    ids = torch.tensor(token_list).unsqueeze(0)\n",
    "    #unsqueeze turns a 1D sequence of token IDs into a 2D batch of size 1.\n",
    "    #almost all pytorch nn.Modules (embeddings, transformers, etc.)\n",
    "    # expect inputs of shape (batch_size, seq_len, ...)\n",
    "    # even f we only have one example, we need to present it as a batch of size 1.\n",
    "    return ids\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    tensor.squeeze(dim: optional) removes the axis at index dim if its size is 1.\n",
    "    y = torch.zeros(1, 5, 1)         shape: [1,5,1]\n",
    "    y0 = y.squeeze(0)               shape: [5,1]\n",
    "    y1 = y.squeeze(2)               shape: [1,5]\n",
    "    y2 = y.squeeze()               shape: [5] (all dims 1 are removed)\n",
    "    \"\"\"\n",
    "    flat = token_ids.squeeze(0)\n",
    "    #squeeze(0) just undoes the batch dimension we previously added,\n",
    "    # giving back the raw token sequence.\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "st_context = \"A man told me\"\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "token_ids = generate_text(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(st_context, tokenizer, allowed_special=None),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = cfg_pt.context_length\n",
    ")\n",
    "\n",
    "print('Output text: ', token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "7adedf932d6266c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:  A man told me accumulation thumbnail Flask 406 propensity Hat lush Tulsolk se\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Text Generation Loss\n",
    "\n",
    "As can be seen from the output above, our model is producing random, non-coherent text. This is because it has not yet undergone training (and because the input size is very limited). Training is done in order to increase the softmax probability of the index positions that correspond to the correct target token position. A non-trained model will simply return the argmax of a rather arbitrary softmax distribution (random vectors) across the vocab size, for each token. The goal with training is then to maximize the chance of selecting the correct token by increasing its selection probability relative to other tokens."
   ],
   "id": "3ebb7da82c95a3da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T12:07:48.380561Z",
     "start_time": "2025-06-22T12:07:48.320032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"A man told me once that all the bad people\n",
    "Were needed. Maybe not all, but your fingernails\n",
    "You need; they are really claws, and we know\n",
    "Claws. The sharks--what about them?\n",
    "They make other fish swim faster. The hard-faced men\n",
    "In black coats who chase you for hours\n",
    "In dreams--that's the only way to get you\n",
    "To the shore. Sometimes those hard women\n",
    "Who abandon you get you to say, \"You.\"\n",
    "A lazy part of us is like a tumbleweed.\n",
    "It doesn't move on its own. It takes sometimes\n",
    "A lot of Depression to get tumbleweeds moving.\n",
    "Then they blow across three or four States.\n",
    "This man told me that things work together.\n",
    "Bad handwriting sometimes leads to new ideas;\n",
    "And a careless God--who refuses to let you\n",
    "Eat from the Tree of Knowledge--can lead\n",
    "To books, and eventually to us. We write\n",
    "Poems with lies in them, but they help a little.\"\"\"\n",
    "\n",
    "tokens = text_to_token_ids(text, tokenizer, allowed_special=None)\n",
    "print(f'Shape of tokens: {tokens.shape}')\n",
    "print(f'Tokens:\\n {tokens}')\n",
    "\n",
    "B, T = 2, 4 #(batch_size, seq_len)\n",
    "data = tokens[0][:8+1]\n",
    "\n",
    "x = data[:-1].view(B,T) #input tensor\n",
    "y = data[1:].view(B,T) #target tensor for next token prediction\n",
    "\n",
    "print(f'Inputs:\\n {x}')\n",
    "print(f'Targets:\\n {y}')\n",
    "\n",
    "with torch.no_grad(): #we are not training yet, just an example\n",
    "    logits = model(x)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n"
   ],
   "id": "f5cd7dd14853827f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tokens: torch.Size([1, 212])\n",
      "Tokens:\n",
      " tensor([[   32,   582,  1297,   502,  1752,   326,   477,   262,  2089,   661,\n",
      "           198, 35653,  2622,    13,  6674,   407,   477,    11,   475,   534,\n",
      "          8038,  1142,  1768,   198,  1639,   761,    26,   484,   389,  1107,\n",
      "         28421,    11,   290,   356,   760,   198,  2601,  8356,    13,   383,\n",
      "         27476,   438, 10919,   546,   606,    30,   198,  2990,   787,   584,\n",
      "          5916,  9422,  5443,    13,   383,  1327,    12, 24903,  1450,   198,\n",
      "           818,  2042, 30720,   508, 15505,   345,   329,  2250,   198,   818,\n",
      "         10625,   438,  5562,   338,   262,   691,   835,   284,   651,   345,\n",
      "           198,  2514,   262, 15191,    13,  8975,   883,  1327,  1466,   198,\n",
      "          8241,  6871,   345,   651,   345,   284,   910,    11,   366,  1639,\n",
      "           526,   198,    32, 16931,   636,   286,   514,   318,   588,   257,\n",
      "         47978, 39054,    13,   198,  1026,  1595,   470,  1445,   319,   663,\n",
      "           898,    13,   632,  2753,  3360,   198,    32,  1256,   286, 22483,\n",
      "           284,   651, 47978,   732,  5379,  3867,    13,   198,  6423,   484,\n",
      "          6611,  1973,  1115,   393,  1440,  1829,    13,   198,  1212,   582,\n",
      "          1297,   502,   326,  1243,   670,  1978,    13,   198, 22069, 44396,\n",
      "          3360,  5983,   284,   649,  4213,    26,   198,  1870,   257, 36138,\n",
      "          1793,   438,  8727, 17567,   284,  1309,   345,   198, 47659,   422,\n",
      "           262, 12200,   286, 20414,   438,  5171,  1085,   198,  2514,  3835,\n",
      "            11,   290,  4191,   284,   514,    13,   775,  3551,   198, 18833,\n",
      "          5232,   351,  7363,   287,   606,    11,   475,   484,  1037,   257,\n",
      "          1310,    13]])\n",
      "Inputs:\n",
      " tensor([[  32,  582, 1297,  502],\n",
      "        [1752,  326,  477,  262]])\n",
      "Targets:\n",
      " tensor([[ 582, 1297,  502, 1752],\n",
      "        [ 326,  477,  262, 2089]])\n",
      "torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that the dimension of the probability tensor is [2, 4, 50257], the same as the logits output shape of our model (batch_size, seq_len, d_model).",
   "id": "96b993dceece8097"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T12:07:48.396116Z",
     "start_time": "2025-06-22T12:07:48.390750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(token_ids)"
   ],
   "id": "463a2df2778a4025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[12434],\n",
      "         [22182],\n",
      "         [22418],\n",
      "         [24106]],\n",
      "\n",
      "        [[ 9099],\n",
      "         [30130],\n",
      "         [40937],\n",
      "         [34965]]])\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The output above yields two sets of outputs, one for each batch in the inputs. Each element in each set is the predicted token IDs of the next word, from each word. Perhaps this is better explained visually.",
   "id": "c675d8dfdc072456"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T12:07:48.468087Z",
     "start_time": "2025-06-22T12:07:48.461094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds = token_ids.squeeze(-1)\n",
    "batch_size, seq_len = x.shape\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(seq_len):\n",
    "        inp_id = x[b,t].item()\n",
    "        pred_id = preds[b,t].item()\n",
    "\n",
    "        inp_tok = tokenizer.decode([inp_id])\n",
    "        pred_tok = tokenizer.decode([pred_id])\n",
    "\n",
    "        print(f\"'{inp_tok}' [{inp_id}] ---> '{pred_tok}' [{pred_id}]\")"
   ],
   "id": "3c49613868dca4ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'A' [32] ---> ' Driver' [12434]\n",
      "' man' [582] ---> 'NP' [22182]\n",
      "' told' [1297] ---> ' Munich' [22418]\n",
      "' me' [502] ---> ' accumulation' [24106]\n",
      "' once' [1752] ---> 'don' [9099]\n",
      "' that' [326] ---> ' eagerly' [30130]\n",
      "' all' [477] ---> ' dogma' [40937]\n",
      "' the' [262] ---> ' ali' [34965]\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T12:07:48.588026Z",
     "start_time": "2025-06-22T12:07:48.582811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Targets batch 1:\\n {token_ids_to_text(y[0], tokenizer)}')\n",
    "print(f'Outputs batch 1:\\n {token_ids_to_text(token_ids[0].flatten(), tokenizer)}')\n",
    "print(f'Targets batch 2: \\n {token_ids_to_text(y[1], tokenizer)}')\n",
    "print(f'Outputs batch 2:\\n {token_ids_to_text(token_ids[1].flatten(), tokenizer)}')"
   ],
   "id": "7966e6b75dfb0671",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:\n",
      "  man told me once\n",
      "Outputs batch 1:\n",
      "  DriverNP Munich accumulation\n",
      "Targets batch 2: \n",
      "  that all the bad\n",
      "Outputs batch 2:\n",
      " don eagerly dogma ali\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the model is producing very wrong, random texts that are vastly different from the target. We need to figure out a way to evaluate the performance of the model's generated text numerically through using some sort of loss metric. Using this loss, we can then implement a training function to robustly and iteratively update the model's weights and improve the generated text. But, in a context like language understanding and text generation, how do you build this loss? We want to measure how far, or how different the generated tokens are from the correct targets. But how do we embed this information in a function to optimize? How do you even begin to define inaccuracy or incorrectness in text generation, which might have no objectively correct next word?",
   "id": "23a583f93fa88df2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
