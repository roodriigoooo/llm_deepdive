{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generative Pretrained Transformer (GPT) ModeL\n",
    "Now that we understand the attention mechanism, one of the core components of traditional LLMs, we can now put this mechanism in the context of other building blocks, which we can then put together to end up with our own GPT model. Up to this point, we have been keeping embedding dimensionality small in the interest of easier learning and understanding. Now we will attempt to scale everything up to a scale comparable to the smallest GPT-2 model (124 million parameters).\n",
    "\n",
    "#### *Language Models are Unsupervised Multitask Learners (Radford et.al, 2019)*\n",
    "This paper first introduced GPT-2, the largest model of which achieved, at the time, state-of-the-art results in 7 out of 8 tested language modeling datasets in a zero-shot setting. It represented a huge step towards building language models that could be accurately characterized as 'competent generalists', rather than 'narrow experts', systems that could perform tasks (sentiment analysis, translation, entity extraction, etc.) without the need to create and label a separate training set for each one.\n",
    "\n",
    "The standard definition of a language model is an unsupervised probability distribution that is fitted over token sequences. Given a corpus of sequences:\n",
    "\n",
    "$$\n",
    "\\{\\,x^{(j)} = (s_1^{(j)}, s_2^{(j)}, \\dots, s_{n_j}^{(j)})\\}_{j=1}^N.\n",
    "$$\n",
    "\n",
    "We maximize the log-likelihood\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{j=1}^N \\log p\\bigl(x^{(j)}\\bigr),\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "p(x) = \\prod_{i=1}^{n} p\\bigl(s_i \\mid s_{<i}\\bigr).\n",
    "$$\n",
    "\n",
    "Recent architectures, like the Transformer with its self-attention, compute and parametrize each conditional $p(s_i \\mid s_{<i})$ and dramatically increase expressivity. Therefore, learning to perform a single task can be ordinarily modeled as an estimation of a distribution $p(\\text{output}\\mid\\text{input})$. A general solver must however also condition on which task to perform $p(\\text{output}\\mid\\text{input, task})$. Up to this point, task conditioning in the context of multitask settings was implemented at an architectural level: task specific encoders and decoders, at an algorithmic level with meta-learning loops, etc. The paper's hypothesis was that **unsupervised multitask learning via pure language modeling was possible.**\n",
    "\n",
    "> When a large language model is trained on a sufficiently large and diverse dataset it is able to perform well across many domains and datasets. [...] high-capacity models trained to maximize the likelihood of a sufficiently varied text corpus begin to learn how to perform a surprising amount of tasks without the need for explicit supervision.\n",
    ">\n",
    ">--<cite>Language Models are Unsupervised Multitask Learners, Radford et.al, 2019</cite>\n"
   ],
   "id": "851dd60dfc944b6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.draw import cfg\n",
    "from transformers.utils.fx import torch_flip\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig124:\n",
    "    vocab_size: int = 50257\n",
    "    context_length: int = 1024\n",
    "    emb_dim: int = 768\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "    dropout: float = 0.1\n",
    "    qkv_bias: bool = False\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT2Dummy(nn.Module):\n",
    "    \"\"\"\n",
    "    A *do nothing* GPT2 scaffold.\n",
    "    We will progressively swap nn.Identity for real implementations.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: GPTConfig124):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim)\n",
    "        self.pos_emb = nn.Embedding(cfg.context_length, cfg.emb_dim)\n",
    "        self.drop_emb = nn.Dropout(cfg.dropout)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [nn.Identity() for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        self.norm = nn.Identity()\n",
    "        self.lm_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, in_idx: torch.Tensor):\n",
    "        \"\"\"\n",
    "        idx: (batch_size, seq_len) tensor of token indices.\n",
    "        :return: logits: (batch_size, seq_len, vocab_size) tensor of logits (unnormalized scores).\n",
    "        \"\"\"\n",
    "        B, T = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(T, device=in_idx.device))\n",
    "        x = self.drop_emb(tok_embeds + pos_embeds)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ],
   "id": "6eb629d4234cf894"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `self.tok_emb`, `self.pos_emb` turn discrete tokens and positions into continuous vectors of size `emb_dim`.\n",
    "- `self.blocks`, collected in a `nn.ModueList` is where the attention & multilayer perceptron layers would normally live. Each block is currently a no-op; it simply returns its input.\n",
    "- `self.norm` is also a placeholder that does nothing. GPT-2 applies a Layernorm after the stack of the blocks.\n",
    "- `nn.Linear` with `bias=False` projects the final hidden states back to vocabulary size so we can compute logits for the next-token predictions.\n",
    "- The forward pass therefore embeds tokens and positions -> adds them -> adds dropout -> passes through blocks -> layer-norm -> linear head. It mirrors the high-level flow of a real GPT-2, just without any real computations inside the blocks."
   ],
   "id": "7ef91b56cab27d36"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
