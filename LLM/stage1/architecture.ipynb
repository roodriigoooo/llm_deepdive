{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generative Pretrained Transformer (GPT) ModeL\n",
    "Now that we understand the attention mechanism, one of the core components of traditional LLMs, we can now put this mechanism in the context of other building blocks, which we can then put together to end up with our own GPT model. Up to this point, we have been keeping embedding dimensionality small in the interest of easier learning and understanding. Now we will attempt to scale everything up to a scale comparable to the smallest GPT-2 model (124 million parameters).\n",
    "\n",
    "#### *Language Models are Unsupervised Multitask Learners (Radford et.al, 2019)*\n",
    "This paper first introduced GPT-2, the largest model of which achieved, at the time state-of-the-art results in 7 out of 8 tested language modeling datasets in a zero-shot setting. It represented a huge step towards building language models that could be accurately characterized as 'competent generalists', rather than 'narrow experts', systems that could perform tasks (sentiment analysis, translation, entity extraction, etc.) without the need to create and label a separate training set for each one.\n"
   ],
   "id": "851dd60dfc944b6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024, #--> max. length of input tokens the model can handle via pos. embeddings.\n",
    "    \"emb_dim\": 768, #--> we transform each token into a 768-dimensional vector.\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, #--> number of transformer blocks in the model\n",
    "    \"dropout\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
