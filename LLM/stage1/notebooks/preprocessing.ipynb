{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:04.220808Z",
     "start_time": "2025-06-23T21:28:03.750959Z"
    }
   },
   "source": [
    "import urllib.request\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"../../../data/the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../the-verdict.txt', <http.client.HTTPMessage at 0x1090b2c10>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:04.233846Z",
     "start_time": "2025-06-23T21:28:04.229554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../../../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ],
   "id": "ce12567de80a87c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:04.298137Z",
     "start_time": "2025-06-23T21:28:04.294039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)\n",
    "\n",
    "# We refrain from making all text lower-case because caps help LLMs distinguish between proper nouns and common nouns, understand sentence structure, and learn to generate text with proper capitalization."
   ],
   "id": "49674fe19f61b9b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:04.354681Z",
     "start_time": "2025-06-23T21:28:04.348863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)\n",
    "\n",
    "#whether or not we encode whitespaces as separate characters or just remove them depends on our application and its requirements.\n",
    "#keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (like python code). Removing them reduces memory and computing requirements."
   ],
   "id": "44981cc313b2d1ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:04.411493Z",
     "start_time": "2025-06-23T21:28:04.406493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ],
   "id": "d1121209e8475642",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:04.473764Z",
     "start_time": "2025-06-23T21:28:04.465546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])\n",
    "\n",
    "# vocabs define how we map each unique word and special character to a unique integer. each unique token is mapped to a unique token ID.\n",
    "# each unique token is added to the vocab in alphabetical order. duplicate tokens are removed."
   ],
   "id": "13c61c5767ba9880",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:04.585732Z",
     "start_time": "2025-06-23T21:28:04.579667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "\n",
    "#starting with a new sample text, we tokenize this new text and use the existing vocabulary to convert the text tokens into token IDs.\n",
    "# the vocab is built from the entire training set and can be applied to the training set itself and any new text samples."
   ],
   "id": "ff4193659cfe27fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**A complete tokenizer class** that:\n",
    "\n",
    "- Splits text into tokens.\n",
    "- Carries out the string-to-integer mapping to produce token IDs via the vocabulary.\n",
    "- Implements a `decode` method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
   ],
   "id": "58570e70074eca2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:04.691396Z",
     "start_time": "2025-06-23T21:28:04.683462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab): # the function works via an existing vocab, which we can use to encode and decode text.\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # preprocesses input text into token IDs\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) # converts tokens back into text\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #removes spaces before the specified punctuation\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"It's the last he painted, you know, Mrs.Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ],
   "id": "d712a004ca4cae7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "It' s the last he painted, you know, Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:04.797379Z",
     "start_time": "2025-06-23T21:28:04.751088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text)) #word \"Hello\" not included in \"The Verdict\" short story. Need to consider larger, diverse training sets to extend the vocabulary when working with LLMs.\n",
    "# we can just modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
    "# we can also add a <|endoftext|> token that we can use to separate two unrelated text sources. this helps the lLM understand that although these text sources are concatenated for training, they are, in fact, unrelated."
   ],
   "id": "7c9907b50b2f972a",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello, do you like tea?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;66;03m#word \"Hello\" not included in \"The Verdict\" short story. Need to consider larger, diverse training sets to extend the vocabulary when working with LLMs.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# we can just modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# we can also add a <|endoftext|> token that we can use to separate two unrelated text sources. this helps the lLM understand that although these text sources are concatenated for training, they are, in fact, unrelated.\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[9], line 9\u001B[0m, in \u001B[0;36mSimpleTokenizerV1.encode\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m      7\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m([,.:;?_!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m()\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m]|--|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m, text) \u001B[38;5;66;03m# preprocesses input text into token IDs\u001B[39;00m\n\u001B[1;32m      8\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m [item\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item\u001B[38;5;241m.\u001B[39mstrip()]\n\u001B[0;32m----> 9\u001B[0m ids \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr_to_int\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpreprocessed\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "Cell \u001B[0;32mIn[9], line 9\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      7\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m([,.:;?_!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m()\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m]|--|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m, text) \u001B[38;5;66;03m# preprocesses input text into token IDs\u001B[39;00m\n\u001B[1;32m      8\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m [item\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item\u001B[38;5;241m.\u001B[39mstrip()]\n\u001B[0;32m----> 9\u001B[0m ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr_to_int\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m preprocessed]\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Hello'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:27.709625Z",
     "start_time": "2025-06-23T21:28:27.703624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ],
   "id": "b8bfbc422497020e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:29.084192Z",
     "start_time": "2025-06-23T21:28:29.074694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab): # the function works via an existing vocab, which we can use to encode and decode text.\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # preprocesses input text into token IDs\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) # converts tokens back into text\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text) #removes spaces before the specified punctuation\n",
    "        return text\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text))) #quick sanity check"
   ],
   "id": "6ad78930052238ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Depending on the LLM, some researchers also consider additional special tokens such as the following:\n",
    "\n",
    "- `[BOS]` (beginning of sequence), marks the start of a text. Signifies to the LLM where a piece of content begins.\n",
    "- `[EOS]` (end of sequence), positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to `<|endoftext|>`.\n",
    "- `[PAD]` (padding), when training LLMs with batch sizes larger than one, the batch might contain texts of varying length. The shorter texts are extended, padded, up to the length of the longest text in the batch.\n"
   ],
   "id": "c84424da459ca1f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:32.226627Z",
     "start_time": "2025-06-23T21:28:31.846291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "#bpe breaks down words into subword units. Unknown tokens are not longer used, nor needed.\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\"Helo, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "        \"of someunknownPlace\")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)\n",
    "\n",
    "#the endoftext token is assigned the largest token ID, out of 50257 tokens.\n",
    "#the BPE encodes and decodes unknown words correctly."
   ],
   "id": "8f37fadc2d8f8ab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12621, 78, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271]\n",
      "Helo, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The algorithm underlying byte-pair encoding breaks down words that aren't in its predefined vocabulary into smaller subwords units or even individual characters, enabling it to handle out-of-vocab words.\n",
    "BPE builds its vocabulary iteratively merging frequent characters into subwords and frequent subwords into words. It starts by adding all individual single characters to its vocab (\"a\", \"b\", etc.). Next, it merges character combinations that frequently occur together into subwords, each merge being determined by a frequency cutoff.\n",
    "\n",
    "**Example:**\n",
    "- The text sample \"Akwir ier\" would first be tokenized into individual characters or subwords.\n",
    "- \"Ak\", \"w\", \"ir\", \"w\", \"\", \"ier\", each with their corresponding token IDs (33901, 86, 343, 86, 220, 959).\n",
    "\n",
    "The ability to break down words into individual characters ensures that the tokenizer, and the LLM that is trained with it, can process any text, even if it contains words that were not present in its training data."
   ],
   "id": "998b2a87959bd8ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:34.360687Z",
     "start_time": "2025-06-23T21:28:34.356309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = \"Awkir ier\"\n",
    "integers_ex = tokenizer.encode(example, allowed_special={\"<|endoftext|>\"})\n",
    "string_ex = tokenizer.decode(integers_ex)\n",
    "print(integers_ex, string_ex)"
   ],
   "id": "64c9efb2f624b334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23155, 74, 343, 220, 959] Awkir ier\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Sampling with a Sliding Window\n",
    "We now create the input-target pairs required for training the LLM. LLMs are pretrained by predicting the next word in a text, and we mask out all words that are past the target.\n",
    "**Example:**\n",
    "\n",
    "- Input: \"LLMs\", target: \"learn\"\n",
    "- Input: \"LLMs learn\", target: \"to\"\n",
    "- Input: \"LLMs learn to\", target: \"predict\"\n",
    "- Input: \"LLMs learn to predict\", target: \"one\"\n",
    "- Input: \"LLMs learn to predict one\", target: \"word\"\n"
   ],
   "id": "f98c9c08576d2586"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:36.354439Z",
     "start_time": "2025-06-23T21:28:36.344341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../../../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4 #how many tokens are included in the input\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ],
   "id": "6bc2ce86cb8426be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:37.941996Z",
     "start_time": "2025-06-23T21:28:37.936641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"--->\", desired)\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"--->\", tokenizer.decode([desired]))"
   ],
   "id": "59e14c2c8dad01ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ---> 4920\n",
      "[290, 4920] ---> 2241\n",
      "[290, 4920, 2241] ---> 287\n",
      "[290, 4920, 2241, 287] ---> 257\n",
      " and --->  established\n",
      " and established --->  himself\n",
      " and established himself --->  in\n",
      " and established himself in --->  a\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As a last step to enable embeddings, we need an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors. We want to return two tensors:\n",
    "\n",
    "- an input tensor containing the text that the LLM sees.\n",
    "- an output, target sensor that includes the targets for the LLM to predict.\n",
    "\n",
    "Each row in the (of size `max_length`) input tensor `x` represents one input context. A second tensor,`y` contains the corresponding prediction targets, which are created by shifting the input by one position."
   ],
   "id": "9ae7f7401c97dfef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:42.147417Z",
     "start_time": "2025-06-23T21:28:40.242887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        token_ids = tokenizer.encode(txt) #tokenizes entire text, and converts them into token IDs as a single step.\n",
    "        self.token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        self.n_windows = (len(token_ids) - self.max_length) // self.stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_windows # returns the total number of rows in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.stride\n",
    "        chunk = self.token_ids[start : start + self.max_length + 1]\n",
    "        return chunk[:-1].clone(), chunk[1:].clone()\n",
    "\n",
    "def create_dataloader(txt,\n",
    "                      tokenizer=None,\n",
    "                      enc_name='gpt2',\n",
    "                      batch_size=4,\n",
    "                      max_length=256,\n",
    "                      stride=128,\n",
    "                      shuffle=True,\n",
    "                      drop_last=True,\n",
    "                      num_workers=0):\n",
    "    tokenizer = tokenizer or tiktoken.get_encoding(enc_name)\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length=max_length, stride=stride)\n",
    "    return DataLoader(dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      drop_last=drop_last, #drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
    "                      num_workers=num_workers) # num of CPU processes to use for preprocessing.\n"
   ],
   "id": "4e6a052d05ab5188",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We test the `dataloader` with a batch size of 1 for an LLM with a context size of 4 to develop an intuition of how the `GPTDataset` class and the `create_dataloader` work together.",
   "id": "7f1a89c449f4f2a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:28:53.765866Z",
     "start_time": "2025-06-23T21:28:53.746335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../../../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader) # convert dataloader into a iterator to fetch the next entry, leveraging the defined __getitem__ function above.\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch[0]) #the input token IDs\n",
    "print(first_batch[1]) #the target token IDs.\n",
    "\n",
    "#Since max_length is set to 4, each of the two tensors contain four token IDs."
   ],
   "id": "641add05822f0e16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  40,  367, 2885, 1464]])\n",
      "tensor([[ 367, 2885, 1464, 1807]])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:29:00.827037Z",
     "start_time": "2025-06-23T21:29:00.821955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch[0])\n",
    "print(second_batch[1])"
   ],
   "id": "d354227c6c045ea2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 367, 2885, 1464, 1807]])\n",
      "tensor([[2885, 1464, 1807, 3619]])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If we compare the first and second batches, we see that the second batch's token IDs are shifted by only one positions. The `stride` parameter dictates the number of positions the inputs shift across batches, emulating a sliding window approach.\n",
    "\n",
    "Note that, in the case below, we increase the stride to 4 to utilize the data set fully. This avoids overlap while using every single word. **More overlap could lead to increased overfitting**.\n"
   ],
   "id": "1a32237952256163"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:29:11.210165Z",
     "start_time": "2025-06-23T21:29:11.200472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs: \\n\", inputs)\n",
    "print(\"Targets: \\n\", targets)"
   ],
   "id": "173caca32a01af4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets: \n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:29:13.113384Z",
     "start_time": "2025-06-23T21:29:13.100372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = torch.tensor([2,3,5,1]) #suppose we have four tokens with IDs 2,3,5 and 1\n",
    "vocab_size = 6 #for simplicity, we have a small vocab of 6 words\n",
    "output_dim = 3 #we want to create embeddings of size 3 (GPT3 has embedding size of 12288)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight) #there is one row for each word, and one column for each of the three dimensions\n",
    "\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "print(embedding_layer(input_ids))"
   ],
   "id": "1f1cfcff6f5c4a60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The embedding layer is essentially a lookup operation retrieving rows from the embedding layer's weight matrix, through token IDs. **The embedding layer is just a more efficient way of implementing one-hot encoding followed by a matrix multiplication in a fully connected layer**. It therefore can be seen as a neural network that can be optimized via backpropagation.\n",
    "\n",
    "So far, the embedding layer maps the same token ID to the same vector representation, regardless of position in the input sequence. Indeed, a minor problem with LLMs is that their self-attention mechanism does not have a notion for position. It is therefore helpful to inject additional position information into the lLM.\n",
    "\n",
    "- **Relative Positional Embeddings**: Emphasis is on the relative position or distance between tokens. The model learns relationships in terms of 'how far apart' rather than 'at which exact position'. The model can usually generalize better to sequences of varying length.\n",
    "- **Absolute Positional Embeddings**: Directly associated with specific positions in a sequence. For each position in an input sequence, a unique embedding is added to the token's embedding to convey its location. Token Embedding: [1,1,1] --> Positional Embedding: [1.1, 1.2, 1.3] --> Input Embeddings: [2.1, 2.2, 2.3].\n",
    "\n",
    "The choice between them depends on the specific application and the nature of the data being processed. OpenAI's models use absolute positional embeddings, but they are optimized in the training process rather than being fixed or predefined."
   ],
   "id": "c103c28d7415a28c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:29:23.101208Z",
     "start_time": "2025-06-23T21:29:22.971155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False) #sampling from the dataloader, we embed each token in each batch into a 256 dim vector.\n",
    "# if we have a batch of 8 with four tokens each, the result will be an 8x4x256 tensor.\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ],
   "id": "c7e6fa09c9921949",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T21:29:25.028428Z",
     "start_time": "2025-06-23T21:29:25.022865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "context_length = max_length #supported input size of the LLM.\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape) #input to the pos_embeddings is usually a placeholder vector torch.arange(context_length) containing a seq. of numbers up to the max.length-1.\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ],
   "id": "177bf816681a23f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now have input embeddings that can be decoded for further postprocessing steps, and eventually to generate output text.\n",
   "id": "ffa5438d2281dfda"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
