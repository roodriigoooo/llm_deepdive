{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## tokenizing with byte-pair encoding\n",
    "\n",
    "### *Neural Machine Translation of Rare Words with Subword Units, Sennrich et. al (2015)*\n",
    "\n",
    "Byte-pair encoding (BPE) is a compression algorithm that was first described by Philip Gage in 1994. This original version of BPE encoded strings of text into smaller strings of text by merging and replacing the highest-frequency contiguous pair of bytes. Byte-pair encoding was then [adapted by Rico Sennrich, Barry Haddow and Alexandra Birch](https://arxiv.org/pdf/1508.07909) to optimize neural machine translation, and namely to solve the problem of translation of rare words. word-level models, relying on complete words, can't possibly generate or translate words they had never seen before, a problem that becomes bigger when dealing with various dialects, alphabets, etc.\n",
    "\n",
    "Sennrich et.al showed that translation of rare words is possible through their encoding via subword units, and additionally created a vocabulary, (a set of tokens, each one of variable-length) that was fixed in size but that could handle any input given to it. the argument was that, the translation of some word is possible and 'transparent' for any competent translator, even if the word is novel or unknown, based on an analysis of known subwords contained within that word (morphemes or phonemes).\n",
    "\n",
    "> a very simplified view of the algorithm:\n",
    ">\n",
    "> - Start with a vocabulary that is just the raw symbols (characters)\n",
    "> - Repeatedly merge the most-common adjacent symbol pairs into new, longer tokens.\n",
    "> - Stop when you hit a fixed vocab size (e.g. 50k)\n",
    "\n",
    "### *Language Models are Unsupervised Multitask Learners, Radford et.al (2019)*\n",
    "\n",
    "in the context of language models, this paper introduced BPE as a mechanism for language models, recognizing and leveraging the many desirable properties that the algorithm had in the context of language modeling. alternatives to BPE included classic word-level tokenisers that, as Sennrich et.al suggested, choken on funky slang, creative spelling, emojis, and anything else they had never seen.\n",
    "\n",
    "> nice compression (one token per known word) but brittle. any typoe or neologism explodes into an `<unk>` or a pile of fallback characters.\n",
    "\n",
    "character-level models fixed this coverage, but exploded sequence length and more easily lost track of higher-level patterns.\n",
    "\n",
    "> antidisestablishmentarianism -> 28 tokens. long sequences, slower training, limits how much context we can put into the model's window.\n",
    "\n",
    "BPE is described to be a middle ground between both: shorter than characters, nimbler than words. It produces fewer tokens than a pure character model (shorter sequences) while staying more adaptable than a pure word model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9cbb49e2e80966f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### implementing byte-pair encoding\n",
    "\n",
    "Strings are sequences of Unicode code points. [Unicode](https://en.wikipedia.org/wiki/Unicode) is a character encoding standard defining more than 150,000 characters and 168 scripts.\n",
    "\n",
    "> From the Python documentation: *Textual data in Python is handled with str objects, or strings. Strings are immutable sequences of Unicode code points.*\n",
    "\n",
    "the vast majority of text available in the internet is encoded using Unicode. The `ord()` function returns the number representing the unicode code of a given character."
   ],
   "id": "81c9ea8f97185f1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:34.400912Z",
     "start_time": "2025-08-05T20:41:34.391732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from docutils.utils.math import pick_math_environment\n",
    "\n",
    "characs = ['áƒš', 'áƒž', 'ðŸŒž', 'ðŸ”¥', 'à¼‚', 'à¼…']\n",
    "for char in characs:\n",
    "    print(f\"Character: {char} -> Unicode code: {ord(char)}\")"
   ],
   "id": "a66a4630dba6b746",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: áƒš -> Unicode code: 4314\n",
      "Character: áƒž -> Unicode code: 4318\n",
      "Character: ðŸŒž -> Unicode code: 127774\n",
      "Character: ðŸ”¥ -> Unicode code: 128293\n",
      "Character: à¼‚ -> Unicode code: 3842\n",
      "Character: à¼… -> Unicode code: 3845\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "computers ultimately read and write bytes, and not abstract code points like the ones shown above. An *encoding* is a mapping turning each code point sequence to a byte sequence (a process called serialization) and back (deserialization). The Unicode Standard defines three main character encoding standards used for electronic communication:\n",
    "\n",
    "- UTF-8 -> 1 byte unit size, variable length (1-4 bytes per code point), typically used in the web, APIs and Unix.\n",
    "- UTF-16 -> 2 bytes unit size (variable length).\n",
    "- UTF-32 -> 4 bytes unit size (fixed-width)\n",
    "\n",
    "almost every single webpage is transmitted as UTF-8, mostly due to the following reasons:\n",
    "- it is the only encoding standard that is backward compatible with ASCII (another encoding standard used in a lot of legacy tooling) meaning that any text file encoded in ASCII can be decoded as UTF-8 to get exactly the same result.\n",
    "- it is space efficient, at least for latin corpora. English text, for instance, stays at around 1 byte per character, whereas UTF-16 or 32 double or quadruple it (see below).\n",
    "\n",
    "> a note on something i was personally confused with:\n",
    ">\n",
    "> - UTF-8 is 'variable-length' because different characters need 1-4 bytes to encode. but **each individual byte** can still store only 0-255 ($2^8 - 1$), being those 256 byte values what a byte-level tokenizer treats as its entire vocabulary. Do not confuse symbols with byte tokens. UTF-8 has 256 distinct byte tokens, which you can combine to do way more than 256 characters."
   ],
   "id": "9e4ef38b1549ed28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:34.508826Z",
     "start_time": "2025-08-05T20:41:34.500267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "samples = {\n",
    "    \"ascii\": \"Hello, world!\",\n",
    "    \"mutlilang\": \"ã“ã‚“ã«ã¡ã¯ä¸–ç•ŒðŸŒ\", # hello world in japanese + emoji\n",
    "    \"emojis\": \"ðŸ”¥ðŸŒžðŸ’§ðŸŒ±\" # emojis\n",
    "}\n",
    "\n",
    "print(\"{name:10} | utfâ€‘8 | utfâ€‘16â€‘le | utfâ€‘32â€‘le | Python str (CPython 3.12) sizeof\")\n",
    "print(\"---------\")\n",
    "for name, s in samples.items():\n",
    "    utf8 = len(s.encode(\"utf-8\"))\n",
    "    u16 = len(s.encode(\"utf-16-le\"))\n",
    "    u32 = len(s.encode(\"utf-32-le\"))\n",
    "    pyobj = getsizeof(s)\n",
    "    print(f\"{name:10} | {utf8:5d} | {u16:10d} | {u32:10d} | {pyobj:27d}\")"
   ],
   "id": "f21f2cf90b87d199",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{name:10} | utfâ€‘8 | utfâ€‘16â€‘le | utfâ€‘32â€‘le | Python str (CPython 3.12) sizeof\n",
      "---------\n",
      "ascii      |    13 |         26 |         52 |                          62\n",
      "mutlilang  |    25 |         18 |         32 |                         108\n",
      "emojis     |    16 |         16 |         16 |                          92\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice that, for ASCII-heavy text (latin-based corpora), UTF-8 is 2x-4x smaller than UTF-18/32. let's show the proportion of zero-valued bytes and preview the first 32 bytes of each encoding:",
   "id": "854bf1c46c52b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:34.735419Z",
     "start_time": "2025-08-05T20:41:34.728935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hex_preview(blob:bytes, limit=32):\n",
    "    head = blob[:limit]\n",
    "    return \" \".join(f\"{b:02x}\" for b in head) + (\" ...\" if len(blob) > limit else \"\")\n",
    "\n",
    "for name, s in samples.items():\n",
    "    print(name.upper())\n",
    "    for enc in (\"utf-8\", \"utf-16-le\", \"utf-32-le\"):\n",
    "        blob = s.encode(enc)\n",
    "        zeros = blob.count(0)\n",
    "        print(f\"{enc:10} | {len(blob):2d} bytes | {zeros:2d} zero bytes | {100*zeros/len(blob):5.1f}% zeros\")\n",
    "        print(\"  \", hex_preview(blob))\n",
    "    print()\n",
    "\n"
   ],
   "id": "4a0d5ee82c2840ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCII\n",
      "utf-8      | 13 bytes |  0 zero bytes |   0.0% zeros\n",
      "   48 65 6c 6c 6f 2c 20 77 6f 72 6c 64 21\n",
      "utf-16-le  | 26 bytes | 13 zero bytes |  50.0% zeros\n",
      "   48 00 65 00 6c 00 6c 00 6f 00 2c 00 20 00 77 00 6f 00 72 00 6c 00 64 00 21 00\n",
      "utf-32-le  | 52 bytes | 39 zero bytes |  75.0% zeros\n",
      "   48 00 00 00 65 00 00 00 6c 00 00 00 6c 00 00 00 6f 00 00 00 2c 00 00 00 20 00 00 00 77 00 00 00 ...\n",
      "\n",
      "MUTLILANG\n",
      "utf-8      | 25 bytes |  0 zero bytes |   0.0% zeros\n",
      "   e3 81 93 e3 82 93 e3 81 ab e3 81 a1 e3 81 af e4 b8 96 e7 95 8c f0 9f 8c 8d\n",
      "utf-16-le  | 18 bytes |  0 zero bytes |   0.0% zeros\n",
      "   53 30 93 30 6b 30 61 30 6f 30 16 4e 4c 75 3c d8 0d df\n",
      "utf-32-le  | 32 bytes | 15 zero bytes |  46.9% zeros\n",
      "   53 30 00 00 93 30 00 00 6b 30 00 00 61 30 00 00 6f 30 00 00 16 4e 00 00 4c 75 00 00 0d f3 01 00\n",
      "\n",
      "EMOJIS\n",
      "utf-8      | 16 bytes |  0 zero bytes |   0.0% zeros\n",
      "   f0 9f 94 a5 f0 9f 8c 9e f0 9f 92 a7 f0 9f 8c b1\n",
      "utf-16-le  | 16 bytes |  0 zero bytes |   0.0% zeros\n",
      "   3d d8 25 dd 3c d8 1e df 3d d8 a7 dc 3c d8 31 df\n",
      "utf-32-le  | 16 bytes |  4 zero bytes |  25.0% zeros\n",
      "   25 f5 01 00 1e f3 01 00 a7 f4 01 00 31 f3 01 00\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "note that each ASCII character in UTF-16-LE introduces one zero byte of padding , while UTF-32-LE introduces three. that is 50% and 75% 'wasted' space respectively, for corporas at least mostly dominated by latin symbols. even more, for emoji-heavy text (which already needs more than 3 bytes in UTF-8), we still pay a 25% overhead moving to UTF-32.\n",
    "\n",
    "### using raw utf-8 bytes as a tokens looks tempting...\n",
    "\n",
    "Using raw UTF-8 bytes as tokens is an option. at the end of the day, it provides us with a tiny embedding table (only 256 possible bytes) while still overcoming the out-of-vocabulary risk. additionally, encoding/decoding any given text with it is trivial, and already implemented for us:"
   ],
   "id": "f64c7596da8836a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:34.754086Z",
     "start_time": "2025-08-05T20:41:34.748833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"to be or not to be\"\n",
    "utf_8_text_bytes = b\"to be or not to be\"\n",
    "print(f\"Byte literal: {utf_8_text_bytes}\")\n",
    "print(f\"Class of byte literal: {type(utf_8_text_bytes)}\")\n",
    "print(f\"Elements of byte literal:\\n {list(utf_8_text_bytes)}\")\n",
    "og_text = utf_8_text_bytes.decode(\"utf-8\")\n",
    "print(f\"Original text: {og_text}\")\n",
    "print(f\"Class of original text: {type(og_text)}\")\n",
    "print(f\"Elements of original text:\\n {list(og_text)}\")"
   ],
   "id": "5e9d6412de1a9fec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte literal: b'to be or not to be'\n",
      "Class of byte literal: <class 'bytes'>\n",
      "Elements of byte literal:\n",
      " [116, 111, 32, 98, 101, 32, 111, 114, 32, 110, 111, 116, 32, 116, 111, 32, 98, 101]\n",
      "Original text: to be or not to be\n",
      "Class of original text: <class 'str'>\n",
      "Elements of original text:\n",
      " ['t', 'o', ' ', 'b', 'e', ' ', 'o', 'r', ' ', 'n', 'o', 't', ' ', 't', 'o', ' ', 'b', 'e']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This would then be a byte-level tokenizer, simply breaking the UTF-8 stream at every individual byte boundary. higher-plane characters stretching over four bytes just means four consecutive byte-tokens--it does not change the size of the vocab itself. as the above introduced papers explain, this would introduce three problems:\n",
    "\n",
    "- **longer sequences**: to represent any given character we need at least 1 byte (every non-latin or non-ASCII code point expanding to 2-4 bytes). this would force us towards very long sequences of tokens. the complexity of transformers, the inner mechanism of a large language model, scales by $L^2$ in sequence length $L$. the longer the inputs, the more attention work, and the more reduced, in terms of symbols, our context window would be.\n",
    "- **tokens do not line up with meaning**: a single emoji, such as âš½ï¸, lands in the model as 2-4 unrelated integers. since we want the network to first learn how bytes compose into code points (or into words and longer grammatical structures) before it can learn anything about semantics, this is a bit of wasted capacity and training time."
   ],
   "id": "f213a7d06b04fdbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:34.823369Z",
     "start_time": "2025-08-05T20:41:34.815076Z"
    }
   },
   "cell_type": "code",
   "source": "list(\"âš½\".encode(\"utf-8\"))",
   "id": "858b0caaef092957",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[226, 154, 189]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- **the very slim vocab hurts compression**: byte-level needs length of word in bytes tokens. while the size of the embedding table shrinks, the computation costs in terms of later activations, positional embeddings, and attention operations grow far faster. in the example below, for instance, a text in japanese explodes from 8 logical characters to 25 byte-tokens, and emojis explode even more.",
   "id": "d1071f7527d56dbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:34.926852Z",
     "start_time": "2025-08-05T20:41:34.921557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def describe(name, s):\n",
    "    b = s.encode(\"utf-8\")\n",
    "    b_tokens = list(b)\n",
    "    cp_tokens = list(s)\n",
    "\n",
    "    print(f\"{name:10} | bytes: {len(b_tokens):2d} tokens | code-points: {len(cp_tokens):2d} tokens\")\n",
    "    print(f\" unique byte-tokens used: {len(set(b_tokens)):3d} / 256\")\n",
    "    print(f\"preview bytes: {b_tokens[:16]}\")\n",
    "    print()\n",
    "\n",
    "for n, s in samples.items():\n",
    "    describe(n, s)"
   ],
   "id": "f739fa3fd591e0d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii      | bytes: 13 tokens | code-points: 13 tokens\n",
      " unique byte-tokens used:  10 / 256\n",
      "preview bytes: [72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33]\n",
      "\n",
      "mutlilang  | bytes: 25 tokens | code-points:  8 tokens\n",
      " unique byte-tokens used:  16 / 256\n",
      "preview bytes: [227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 228]\n",
      "\n",
      "emojis     | bytes: 16 tokens | code-points:  4 tokens\n",
      " unique byte-tokens used:   9 / 256\n",
      "preview bytes: [240, 159, 148, 165, 240, 159, 140, 158, 240, 159, 146, 167, 240, 159, 140, 177]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "let's see a very basic approximation of the additional compute associated with this explosion:",
   "id": "a3576420a7340718"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:35.064421Z",
     "start_time": "2025-08-05T20:41:35.059587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transformer_flops(n_layers, dim, seq_len, factor=2):\n",
    "    return n_layers * factor * (seq_len ** 2) * dim\n",
    "\n",
    "L  = 12           # layers\n",
    "H  = 768          # hidden size\n",
    "char_len = 8 # japanese text numbers\n",
    "byte_len = 25\n",
    "\n",
    "print(\"Char-level  :\", f\"{transformer_flops(L, H, char_len):,.2f} floating point operations needed.\")\n",
    "print(\"Byte-level  :\", f\"{transformer_flops(L, H, byte_len):,.2f} floating point operations needed.\")\n"
   ],
   "id": "a72ba75486153942",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char-level  : 1,179,648.00 floating point operations needed.\n",
      "Byte-level  : 11,520,000.00 floating point operations needed.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### utf-8 aware $\\ne$ byte-bound\n",
    "\n",
    "We want to start and keep UTF-8 as a base because it gives us three very important guarantees: most web-based text and any future Unicode character already has a UTF-8 spelling (our tokenizer would never go out of date, unless the Unicode standard is replaced), we have a method for loss-free round-trips (text -> tokens -> text is always possible and exact) and UTF-8 can be processed left-to-right without backtracking, good for data pipelines, distributed and sharded datasets.\n",
    "\n",
    "therefore, using the same 256-byte alphabet, we want to apply some sort of clever operations on top that shrinks average sequence length, caps vocab at a manageable size, remains fully reversible and stays future-proof."
   ],
   "id": "1c015e412b7ca1d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### actually implementing byte-pair encoding\n",
    "\n",
    "let's use morse code to see Byte-Pair encoding in action. the morse code alphabet is composed only of two symbols (and the space), composed each of a single byte."
   ],
   "id": "aa5450c71fc37a64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:35.172962Z",
     "start_time": "2025-08-05T20:41:35.167449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "morse_alphabet = [\".\", \"-\"]\n",
    "for token in morse_alphabet:\n",
    "    print(f\"Byte-encodings of morse code ({token}): {list(token.encode('utf-8'))}\")"
   ],
   "id": "b9a640e93cb32e25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte-encodings of morse code (.): [46]\n",
      "Byte-encodings of morse code (-): [45]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "suppose for instance we wanted to encode ['hello hello' in Morse](https://morsecode.world/international/translator.html):\n",
    "\n",
    "``` bash\n",
    ".... . .-.. .-.. --- .... . .-.. .-.. ---\n",
    "\n",
    "# hello hello in morse\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "6e857f52399c4ff0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:35.275360Z",
     "start_time": "2025-08-05T20:41:35.269982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hello_hello = \".... . .-.. .-.. --- .... . .-.. .-.. ---\"\n",
    "\n",
    "# encode to raw bytes\n",
    "byte_tokens = list(hello_hello.encode(\"utf-8\"))\n",
    "unique_byte_tokens = set(byte_tokens)\n",
    "\n",
    "# map each byte back to its printable char\n",
    "unique_chars = [bytes([b]).decode(\"utf-8\") for b in unique_byte_tokens]\n",
    "\n",
    "\n",
    "print(f\"Raw byte list:\\n{byte_tokens}\\n\")\n",
    "print(f\"Sequence length: {len(byte_tokens)}\")\n",
    "print(f\"Unique byte tokens: {len(unique_byte_tokens)} â†’ {unique_byte_tokens}\")\n",
    "print(f\"Unique char tokens: {len(unique_chars)}   â†’ {unique_chars}\")"
   ],
   "id": "7b52c10136ad32d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw byte list:\n",
      "[46, 46, 46, 46, 32, 46, 32, 46, 45, 46, 46, 32, 46, 45, 46, 46, 32, 45, 45, 45, 32, 46, 46, 46, 46, 32, 46, 32, 46, 45, 46, 46, 32, 46, 45, 46, 46, 32, 45, 45, 45]\n",
      "\n",
      "Sequence length: 41\n",
      "Unique byte tokens: 3 â†’ {32, 45, 46}\n",
      "Unique char tokens: 3   â†’ [' ', '-', '.']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "so the Morse-encodeed 'hello hello' uses three distincy byte tokens only. that is a very small vocab: the embedding table size is very small, but sequence length explodes (41 symbols for just two words). this is exactly the trade-off we described earlier.\n",
    "\n",
    "to fix this, the BPE iteratively finds the most common adjacent pair of characters, fuses them into a single, new glyph and replaces every occurrence. here we find the most common adjacent pairs of characters:"
   ],
   "id": "2a2fa5584848c656"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:35.340484Z",
     "start_time": "2025-08-05T20:41:35.334936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(zip(hello_hello, hello_hello[1:]))\n",
    "print(\"Counts of adjacent pairs:\")\n",
    "for c in counts:\n",
    "    print(f\"\\n{c}, {counts[c]}\")\n",
    "    ((a, b)), freq = counts.most_common(1)[0]\n",
    "print(\"Most common pair:\")\n",
    "print(f\"'{a}{b}' with freq: {freq}\") if freq > 1 else None  # stop when every pair is unique"
   ],
   "id": "a9018a89b27a6ca9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of adjacent pairs:\n",
      "\n",
      "('.', '.'), 10\n",
      "\n",
      "('.', ' '), 8\n",
      "\n",
      "(' ', '.'), 7\n",
      "\n",
      "('.', '-'), 4\n",
      "\n",
      "('-', '.'), 4\n",
      "\n",
      "(' ', '-'), 2\n",
      "\n",
      "('-', '-'), 4\n",
      "\n",
      "('-', ' '), 1\n",
      "Most common pair:\n",
      "'..' with freq: 10\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:35.420077Z",
     "start_time": "2025-08-05T20:41:35.412889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# helper function\n",
    "def most_common_pair(seq):\n",
    "    counts = Counter(zip(seq, seq[1:]))\n",
    "    ((a, b)), freq = counts.most_common(1)[0]\n",
    "    return (a, b, freq) if freq > 1 else None  # stop when every pair is unique\n",
    "\n",
    "most_common_pair(list(hello_hello))"
   ],
   "id": "57f9201682f66142",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', '.', 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "then, we mint a new glyph using the most common pair, and we replace every occurrence of it. For example, we mint a new symbol called 'A', and 'A' replaces every '..'. For instance, the starting symbols `....` become `AA`.",
   "id": "3dbe29378ce184d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:35.503803Z",
     "start_time": "2025-08-05T20:41:35.496654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fuse_once(seq, pair, new_sym):\n",
    "    a, b = pair\n",
    "    out, i = [], 0\n",
    "    while i < len(seq):\n",
    "        if i + 1 < len(seq) and seq[i]==a and seq[i+1]==b:\n",
    "            out.append(new_sym); i += 2\n",
    "        else:\n",
    "            out.append(seq[i]);  i += 1\n",
    "    return out\n",
    "\n",
    "vocabulary = {'.', '-'}\n",
    "rules = []\n",
    "\n",
    "pair_found = most_common_pair(list(hello_hello))\n",
    "a, b, freq = pair_found\n",
    "rules.append(f\"A = {a}{b}\")\n",
    "vocabulary.add('A')\n",
    "\n",
    "new_hello_hello = fuse_once(hello_hello, (a,b), \"A\")\n",
    "print(f\"New String: {new_hello_hello}\")\n",
    "print(f\"Rule: {rules}\")"
   ],
   "id": "de27bc221114866a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New String: ['A', 'A', ' ', '.', ' ', '.', '-', 'A', ' ', '.', '-', 'A', ' ', '-', '-', '-', ' ', 'A', 'A', ' ', '.', ' ', '.', '-', 'A', ' ', '.', '-', 'A', ' ', '-', '-', '-']\n",
      "Rule: ['A = ..']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "the symbol `A` then becomes part of the vocabulary, and the same process is run iteratively until we have hit a predetermined number of tokens, or alternatively until the remaining bytes have adjacent pairs that do not occur more than once. the next iteration then would look like this.",
   "id": "416ea0477d8aa03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:35.556346Z",
     "start_time": "2025-08-05T20:41:35.551425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pair_found = most_common_pair(list(new_hello_hello))\n",
    "a, b, freq = pair_found\n",
    "rules.append(f\"B = {a}{b}\")\n",
    "vocabulary.add('B')\n",
    "\n",
    "new_hello_hello = fuse_once(new_hello_hello, (a,b), \"B\")\n",
    "print(f\"New String: {new_hello_hello}\")\n",
    "print(f\"Rule: {rules}\")"
   ],
   "id": "79733f645b6c02db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New String: ['A', 'B', '.', ' ', '.', '-', 'B', '.', '-', 'B', '-', '-', '-', ' ', 'A', 'B', '.', ' ', '.', '-', 'B', '.', '-', 'B', '-', '-', '-']\n",
      "Rule: ['A = ..', 'B = A ']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "so the sequence `A `, (an A followed by a space, or `.. `) gets replaced by a new symbol `B` which is then added to our vocabulary. running this algorithm iteratively until the end would result in something like the following:",
   "id": "2a07ca7f53b28867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:35.623137Z",
     "start_time": "2025-08-05T20:41:35.614351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "\n",
    "vocabulary = {'.', '-', ' '}\n",
    "rules    = []\n",
    "fresh    = iter(string.ascii_uppercase)\n",
    "step     = 1\n",
    "\n",
    "print(\"Start :\", \"\".join(hello_hello))\n",
    "while True:\n",
    "    found = most_common_pair(hello_hello)\n",
    "    if not found:\n",
    "        break\n",
    "    a, b, freq = found\n",
    "    new_sym    = next(fresh)          # mint A, B, C, ...\n",
    "    rules.append(f\"{new_sym} = {a}{b}\")\n",
    "    vocabulary.add(new_sym)\n",
    "    hello_hello = fuse_once(hello_hello, (a, b), new_sym)\n",
    "\n",
    "    print(f\"Step {step:>2} : replace '{a}{b}' â†’ '{new_sym}' \"\n",
    "          f\"(appeared {freq}Ã—)  â†’  {''.join(hello_hello)}\")\n",
    "    step += 1\n",
    "\n",
    "print(\"\\nCompressed sequence :\", \"\".join(hello_hello))\n",
    "print(\"Merge rules (oldest â†’ newest):\")\n",
    "for r in rules:\n",
    "    print(\" \", r)"
   ],
   "id": "987fee7b63e80d93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : .... . .-.. .-.. --- .... . .-.. .-.. ---\n",
      "Step  1 : replace '..' â†’ 'A' (appeared 10Ã—)  â†’  AA . .-A .-A --- AA . .-A .-A ---\n",
      "Step  2 : replace 'A ' â†’ 'B' (appeared 6Ã—)  â†’  AB. .-B.-B--- AB. .-B.-B---\n",
      "Step  3 : replace 'B.' â†’ 'C' (appeared 4Ã—)  â†’  AC .-C-B--- AC .-C-B---\n",
      "Step  4 : replace '--' â†’ 'D' (appeared 4Ã—)  â†’  AC .-C-BD- AC .-C-BD-\n",
      "Step  5 : replace 'AC' â†’ 'E' (appeared 2Ã—)  â†’  E .-C-BD- E .-C-BD-\n",
      "Step  6 : replace 'E ' â†’ 'F' (appeared 2Ã—)  â†’  F.-C-BD- F.-C-BD-\n",
      "Step  7 : replace 'F.' â†’ 'G' (appeared 2Ã—)  â†’  G-C-BD- G-C-BD-\n",
      "Step  8 : replace 'G-' â†’ 'H' (appeared 2Ã—)  â†’  HC-BD- HC-BD-\n",
      "Step  9 : replace 'HC' â†’ 'I' (appeared 2Ã—)  â†’  I-BD- I-BD-\n",
      "Step 10 : replace 'I-' â†’ 'J' (appeared 2Ã—)  â†’  JBD- JBD-\n",
      "Step 11 : replace 'JB' â†’ 'K' (appeared 2Ã—)  â†’  KD- KD-\n",
      "Step 12 : replace 'KD' â†’ 'L' (appeared 2Ã—)  â†’  L- L-\n",
      "Step 13 : replace 'L-' â†’ 'M' (appeared 2Ã—)  â†’  M M\n",
      "\n",
      "Compressed sequence : M M\n",
      "Merge rules (oldest â†’ newest):\n",
      "  A = ..\n",
      "  B = A \n",
      "  C = B.\n",
      "  D = --\n",
      "  E = AC\n",
      "  F = E \n",
      "  G = F.\n",
      "  H = G-\n",
      "  I = HC\n",
      "  J = I-\n",
      "  K = JB\n",
      "  L = KD\n",
      "  M = L-\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "to be clear, the vocabulary keeps every symbol that was minted along the way. The final sequence being `M M` reflects (apart from the original string being a repetition of the same structure with a space in between, namely 'hello hello') that the final sequence shrank to two tokens (`M`, `space`). The embedding table still has the full amount of original and generated entries in the vocabulary:",
   "id": "ef94744622402851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:35.687418Z",
     "start_time": "2025-08-05T20:41:35.681726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Final compressed sequence:\", ''.join(hello_hello))\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"Vocabulary symbols:\", sorted(vocabulary))\n"
   ],
   "id": "5a18cf4f950eb2da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final compressed sequence: M M\n",
      "Vocabulary size: 16\n",
      "Vocabulary symbols: [' ', '-', '.', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "More importantly, during BPE training, we decide when to stop merging. using the example above, starting out from our original vocabulary of `['.', '-', ' ']`, we can decide that we want our final vocabulary size to be of 10, and therefore we would end after the 7th step, being our final vocabulary `['.', '-', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G']`. the earlier we stop it, the more tiny our vocabulary is, and the longer our long token sequences. conversely, if we merge until there is nothing else to merge, our vocabulary grows in size, but we have ultra-short sequences.",
   "id": "38278bcefeae4283"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It is also important to note that the example thus far has used characters directly to perform the byte-pair encoding (we should be referring to it so far as character-pair encoding, really...). in our case, we get away with it because all of the characters in the original corpora (`hello hello` in Morse) are single-byte characters (see above). however, remember that UTF-8 encodes Unicode characters into up to 4 bytes. *byte* pair encoding never sees characters, it sees raw byte values (0-255) only.\n",
    "\n",
    "### byte pair encoding for a larger corpora\n",
    "\n",
    "i decided to use the [Gutenberg Poetry Corpus](https://github.com/aparrish/gutenberg-poetry-corpus) by Allison Parish. it is composed of more than 3 million lines of poetry obtained from publicly available books in [Project Gutenberg](https://gutenberg.org/). The `line` column are single lines of poetry, while the `gutenberg_id` is the ID of the Project Gutenberg book that the line is coming from."
   ],
   "id": "4949c0ba40cf7a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:41:52.802054Z",
     "start_time": "2025-08-05T20:41:35.798296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"biglam/gutenberg-poetry-corpus\", split=\"train\", streaming=True)\n",
    "\n",
    "# streaming=True is set to work with the dataset without downloading it. rather, the data is streamed as we need it (as we iterate over the dataset)."
   ],
   "id": "597eb714e5f21e52",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:42:09.781610Z",
     "start_time": "2025-08-05T20:42:09.776072Z"
    }
   },
   "cell_type": "code",
   "source": "print(ds)",
   "id": "680117eb3d0cd2f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['line', 'gutenberg_id'],\n",
      "    num_shards: 1\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "let's take 100k random lines as a start:",
   "id": "f459c4fb848a8283"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:42:40.675737Z",
     "start_time": "2025-08-05T20:42:11.166674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "sample = list(itertools.islice(ds.shuffle(buffer_size=100_000, seed=42), 100_000))\n",
    "corpus = [list(row[\"line\"].encode(\"utf-8\")) for row in sample] # raw bytes"
   ],
   "id": "6ad6f8fa95c626b7",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:42:40.691977Z",
     "start_time": "2025-08-05T20:42:40.684476Z"
    }
   },
   "cell_type": "code",
   "source": "corpus[:5] #5 poetry lines selected at random --> their raw bytes",
   "id": "c7167825877460a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[83,\n",
       "  111,\n",
       "  32,\n",
       "  116,\n",
       "  104,\n",
       "  114,\n",
       "  101,\n",
       "  97,\n",
       "  116,\n",
       "  110,\n",
       "  39,\n",
       "  100,\n",
       "  32,\n",
       "  104,\n",
       "  101,\n",
       "  101,\n",
       "  44,\n",
       "  32,\n",
       "  98,\n",
       "  117,\n",
       "  116,\n",
       "  32,\n",
       "  83,\n",
       "  65,\n",
       "  84,\n",
       "  65,\n",
       "  78,\n",
       "  32,\n",
       "  116,\n",
       "  111,\n",
       "  32,\n",
       "  110,\n",
       "  111,\n",
       "  32,\n",
       "  116,\n",
       "  104,\n",
       "  114,\n",
       "  101,\n",
       "  97,\n",
       "  116,\n",
       "  115],\n",
       " [83,\n",
       "  111,\n",
       "  32,\n",
       "  97,\n",
       "  115,\n",
       "  32,\n",
       "  116,\n",
       "  104,\n",
       "  101,\n",
       "  105,\n",
       "  32,\n",
       "  98,\n",
       "  101,\n",
       "  32,\n",
       "  111,\n",
       "  102,\n",
       "  32,\n",
       "  100,\n",
       "  111,\n",
       "  117,\n",
       "  98,\n",
       "  108,\n",
       "  101,\n",
       "  32,\n",
       "  101,\n",
       "  110,\n",
       "  116,\n",
       "  101,\n",
       "  110,\n",
       "  116,\n",
       "  101,\n",
       "  58],\n",
       " [79,\n",
       "  110,\n",
       "  101,\n",
       "  32,\n",
       "  114,\n",
       "  111,\n",
       "  119,\n",
       "  32,\n",
       "  111,\n",
       "  102,\n",
       "  32,\n",
       "  114,\n",
       "  101,\n",
       "  100,\n",
       "  32,\n",
       "  110,\n",
       "  111,\n",
       "  115,\n",
       "  116,\n",
       "  114,\n",
       "  105,\n",
       "  108,\n",
       "  115,\n",
       "  32,\n",
       "  116,\n",
       "  104,\n",
       "  97,\n",
       "  116,\n",
       "  32,\n",
       "  115,\n",
       "  99,\n",
       "  101,\n",
       "  110,\n",
       "  116,\n",
       "  32,\n",
       "  98,\n",
       "  97,\n",
       "  116,\n",
       "  116,\n",
       "  108,\n",
       "  101,\n",
       "  45,\n",
       "  102,\n",
       "  117,\n",
       "  109,\n",
       "  101,\n",
       "  115,\n",
       "  46],\n",
       " [84,\n",
       "  111,\n",
       "  32,\n",
       "  116,\n",
       "  121,\n",
       "  114,\n",
       "  97,\n",
       "  110,\n",
       "  116,\n",
       "  115,\n",
       "  32,\n",
       "  111,\n",
       "  116,\n",
       "  104,\n",
       "  101,\n",
       "  114,\n",
       "  115,\n",
       "  32,\n",
       "  104,\n",
       "  97,\n",
       "  118,\n",
       "  101,\n",
       "  32,\n",
       "  116,\n",
       "  104,\n",
       "  101,\n",
       "  105,\n",
       "  114,\n",
       "  32,\n",
       "  99,\n",
       "  111,\n",
       "  117,\n",
       "  110,\n",
       "  116,\n",
       "  114,\n",
       "  121,\n",
       "  32,\n",
       "  115,\n",
       "  111,\n",
       "  108,\n",
       "  100,\n",
       "  44],\n",
       " [73,\n",
       "  110,\n",
       "  32,\n",
       "  97,\n",
       "  108,\n",
       "  116,\n",
       "  97,\n",
       "  114,\n",
       "  45,\n",
       "  119,\n",
       "  105,\n",
       "  115,\n",
       "  101,\n",
       "  44,\n",
       "  32,\n",
       "  97,\n",
       "  32,\n",
       "  115,\n",
       "  116,\n",
       "  97,\n",
       "  116,\n",
       "  101,\n",
       "  108,\n",
       "  121,\n",
       "  32,\n",
       "  112,\n",
       "  105,\n",
       "  108,\n",
       "  101,\n",
       "  32,\n",
       "  116,\n",
       "  104,\n",
       "  101,\n",
       "  121,\n",
       "  32,\n",
       "  114,\n",
       "  101,\n",
       "  97,\n",
       "  114,\n",
       "  59]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:42:41.012927Z",
     "start_time": "2025-08-05T20:42:40.740140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import chain\n",
    "\n",
    "corpus_lines_str = [row[\"line\"] for row in sample]\n",
    "total_chars = sum(len(line) for line in corpus_lines_str)\n",
    "unique_chars = {ch for line in corpus_lines_str for ch in line}\n",
    "\n",
    "# chain takes a series of iterables and returns one iterable\n",
    "\n",
    "byte_seq = list(chain.from_iterable(corpus))\n",
    "lines_sampled = len(corpus)\n",
    "total_byte_number = len(byte_seq)\n",
    "unique_byte_number = len(set(byte_seq))\n",
    "\n",
    "print(f\"Lines sampled: {lines_sampled}\")\n",
    "print(f\"Total **bytes**: {total_byte_number:,}\")\n",
    "print(f\"Total unique bytes: {unique_byte_number}\")\n",
    "print()\n",
    "print(f\"Total **characters** : {total_chars:,}\")\n",
    "print(f\"Unique characters num : {len(unique_chars):,}\")\n",
    "print(f\"Avg bytes per char:{total_byte_number/total_chars:4.2f}\")"
   ],
   "id": "e56e507243883042",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines sampled: 100000\n",
      "Total **bytes**: 3,836,123\n",
      "Total unique bytes: 101\n",
      "\n",
      "Total **characters** : 3,836,075\n",
      "Unique characters num : 99\n",
      "Avg bytes per char:1.00\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, in 100,000 randomly selected poetry lines, there is a total of 3,836,123 bytes, out of which only 101 are unique. That means that only 101/256 $\\approx$ 40% of the possible byte codes ever appear. but as we already know, even though our memory footprint is excellent, the numbers also exhibit the massive sequence redundancy we are now familiar with.\n",
    "\n",
    "An avg. byte per char of $\\approx$ 1 indicates that the corpus is almost pure ASCII, and that there are pretty much no multi-byte UTF-8 characters to blame for length. what inflates the sequence is therefore not at all UTF-8 overhead, but rather simply the decision to start with single-byte tokens (we have 3,836,123 bytes out of only 101 unique ones!!!). we need to dramatically reduce compute. as i mentioned earlier, the cost of having more vocabulary is linear, while that of having longer sequence lengths (which are operated on by the transformer) is quadratic.\n",
    "\n",
    "let's take better care of our vocabulary, and spend more memory on it, by using our byte pair algorithm on it:"
   ],
   "id": "3bc60fdfb8f6fa41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T20:45:52.650470Z",
     "start_time": "2025-08-05T20:45:52.646304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = Path(\"bpe_cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "BYTE_VOCAB_SIZE = 256 #UTF8"
   ],
   "id": "c25584c2ac2f7217",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T21:00:25.122096Z",
     "start_time": "2025-08-05T21:00:25.106938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle, json, numpy as np\n",
    "\n",
    "def most_common_pair(seq):\n",
    "    counts = Counter(zip(seq, seq[1:]))\n",
    "    ((a, b)), freq = counts.most_common(1)[0]\n",
    "    return (a, b, freq) if freq > 1 else None  # stop when every pair is unique\n",
    "\n",
    "def fuse_once(seq, pair, new_sym):\n",
    "    a, b = pair\n",
    "    out, i = [], 0\n",
    "    while i < len(seq):\n",
    "        if i + 1 < len(seq) and seq[i]==a and seq[i+1]==b:\n",
    "            out.append(new_sym); i += 2\n",
    "        else:\n",
    "            out.append(seq[i]);  i += 1\n",
    "    return out\n",
    "\n",
    "def train_bpe(corpus, max_merges: int, record_every:int = 5):\n",
    "    if isinstance(corpus, list) and all(isinstance(x, int) for x in corpus):\n",
    "        seq = corpus[:]\n",
    "    else:\n",
    "        seq = list(chain.from_iterable(corpus))\n",
    "\n",
    "    vocab = set(range(BYTE_VOCAB_SIZE))\n",
    "    merges, history = [], []\n",
    "    next_id = BYTE_VOCAB_SIZE # so our first added merge will be 256\n",
    "\n",
    "    for i in range(max_merges):\n",
    "        pair = most_common_pair(seq)\n",
    "        if not pair:\n",
    "            break\n",
    "        a, b, _ = pair # dont really need the freq\n",
    "        new_sym = next_id; next_id += 1\n",
    "        merges.append((a, b))\n",
    "        vocab.add(new_sym)\n",
    "        seq = fuse_once(seq, (a, b), new_sym)\n",
    "\n",
    "        if i % record_every == 0 or i == max_merges-1:\n",
    "            history.append({\n",
    "                \"merge_step\": i+1,\n",
    "                \"seq_len\": len(seq),\n",
    "                \"vocab_size\": len(vocab)\n",
    "            })\n",
    "\n",
    "    return merges, seq, vocab, history\n",
    "\n",
    "def run_and_cache(name, corpus, max_merges:int, **kwargs):\n",
    "    \"\"\"\n",
    "    this is just for me to run the bpe 'trainer' once, cache it to disk and return everything.\n",
    "    \"\"\"\n",
    "    stem = f\"{name}_{max_merges}\"\n",
    "    files = {\n",
    "        \"merges\": CACHE_DIR/ f\"{stem}_merges.pkl\",\n",
    "        \"compressed\": CACHE_DIR/ f\"{stem}_seq.npy\",\n",
    "        \"vocab\": CACHE_DIR/ f\"{stem}_vocab.json\",\n",
    "        \"history\": CACHE_DIR/ f\"{stem}_history.json\",\n",
    "    }\n",
    "\n",
    "    # if everything is already on disk\n",
    "    if all(p.exists() for p in files.values()):\n",
    "        with open(files[\"merges\"], \"rb\") as f: merges = pickle.load(f)\n",
    "        seq = np.load(files[\"compressed\"])\n",
    "        with open(files[\"vocab\"]) as f: vocab = set(json.load(f))\n",
    "        with open(files[\"history\"]) as f: history = json.load(f)\n",
    "        return merges, list(seq), vocab, history\n",
    "\n",
    "    merges, seq, vocab, history = train_bpe(corpus, max_merges, **kwargs)\n",
    "\n",
    "    # dump everything\n",
    "    with open(files[\"merges\"], \"wb\") as f: pickle.dump(merges, f, protocol=5)\n",
    "    np.save(files[\"compressed\"], np.array(seq, dtype=np.uint32))\n",
    "    with open(files[\"vocab\"],   \"w\") as f: json.dump(sorted(vocab), f)\n",
    "    with open(files[\"history\"], \"w\") as f: json.dump(history, f, indent=2)\n",
    "\n",
    "    return merges, seq, vocab, history\n",
    "    "
   ],
   "id": "7227295aba491c4a",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T21:07:45.633908Z",
     "start_time": "2025-08-05T21:00:28.100469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max = 500\n",
    "experiments = {}\n",
    "\n",
    "merges, seq, vocab, history = run_and_cache(\"poetry\", corpus, max, record_every=5)\n",
    "experiments[max] = {\"seq\":seq, \"vocab\":vocab, \"history\":history}\n",
    "print(f\"{max:4d} merges âžœ {len(seq):,} tokens, vocab {len(vocab)}\")\n"
   ],
   "id": "97a00749a2515cb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500 merges âžœ 1,692,762 tokens, vocab 756\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T21:11:02.036477Z",
     "start_time": "2025-08-05T21:11:01.892586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merges, seq, vocab, history = run_and_cache(\"poetry\", corpus, max, record_every=5)\n",
    "print(f\"amount of merges learned : {len(merges):,}\")\n",
    "print(f\"amount of compressed tokens : {len(seq):,}\")\n",
    "print(f\"size of final vocabulary: {len(vocab)}\")\n",
    "print(f\"final vocabulary:\\n {vocab}\")"
   ],
   "id": "9de819097ee2e164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of merges learned : 500\n",
      "amount of compressed tokens : 1,692,762\n",
      "size of final vocabulary: 756\n",
      "final vocabulary:\n",
      " {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755}\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Running our BPE algorithm for 500 merges yields a final vocabulary size of 756 (the 256 original ones + 500 new ones). notice also the new sequence length, down from 3.8 million to around 1.7 million. but we need to be precise about what was compressed here and why we care.\n",
    "\n",
    "- the 1.7 million is token count, and not byte file size. before BPE mostly every byte was a separate model (ASCII-based corpora), and the model needed to process a sequence 3.8 million bytes long.\n",
    "- after 500 merges, we introduced 500 new symbols; the sequence now contains only 1.7 million tokens.\n",
    "- we eliminated ~55% of the token-level redundancy the model would otherwise see. but **we did not shrink the file on disk**. actually, storing each token id as a 32-bit integer (4 bytes), the 'compressed' representation is actually bigger (1.7 million x 4 > 3.8).\n",
    "\n",
    "just remembers that BPE is not a storage compression scheme, it is a computational one. it trades a modest increase in vocabulary size for a large drop in sequence length. that trade is profitable because modern autoregressive models are sequence-length bound.\n",
    "\n"
   ],
   "id": "7fec4b98613cd8d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
