{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## tokenizing with byte-pair encoding\n",
    "\n",
    "### *Neural Machine Translation of Rare Words with Subword Units, Sennrich et. al (2015)*\n",
    "\n",
    "[Byte-pair encoding](https://arxiv.org/pdf/1508.07909) (BPE) is a compression algorithm that was first introduced by Rico Sennrich, Barry Haddow and Alexandra Birch to optimize neural machine translation, and namely to solve the problem of translation of rare words. word-level models, relying on complete words, can't possibly generate or translate words they had never seen before, a problem that becomes bigger when dealing with various dialects, alphabets, etc.\n",
    "\n",
    "Sennrich et.al showed that translation of rare words is possible through their encoding via subword units, and additionally created a vocabulary, (a set of tokens, each one of variable-length) that was fixed in size but that could handle any input given to it. the argument was that, the translation of some word is possible and 'transparent' for any competent translator, even if the word is novel or unknown, based on an analysis of known subwords contained within that word (morphemes or phonemes).\n",
    "\n",
    "> a very simplified view of the algorithm:\n",
    ">\n",
    "> - Start with a vocabulary that is just the raw symbols (characters)\n",
    "> - Repeatedly merge the most-common adjacent symbol pairs into new, longer tokens.\n",
    "> - Stop when you hit a fixed vocab size (e.g. 50k)\n",
    "\n",
    "### *Language Models are Unsupervised Multitask Learners, Radford et.al (2019)*\n",
    "\n",
    "in the context of language models, this paper introduced BPE as a mechanism for language models, recognizing and leveraging the many desirable properties that the algorithm had in the context of language modeling. alternatives to BPE included classic word-level tokenisers that, as Sennrich et.al suggested, choken on funky slang, creative spelling, emojis, and anything else they had never seen.\n",
    "\n",
    "> nice compression (one token per known word) but brittle. any typoe or neologism explodes into an `<unk>` or a pile of fallback characters.\n",
    "\n",
    "character-level models fixed this coverage, but exploded sequence length and more easily lost track of higher-level patterns.\n",
    "\n",
    "> antidisestablishmentarianism -> 28 tokens. long sequences, slower training, limits how much context we can put into the model's window.\n",
    "\n",
    "BPE is described to be a middle ground between both: shorter than characters, nimbler than words. It produces fewer tokens than a pure character model (shorter sequences) while staying more adaptable than a pure word model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9cbb49e2e80966f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### implementing byte-pair encoding\n",
    "\n",
    "Strings are sequences of Unicode code points. [Unicode](https://en.wikipedia.org/wiki/Unicode) is a character encoding standard defining more than 150,000 characters and 168 scripts.\n",
    "\n",
    "> From the Python documentation: *Textual data in Python is handled with str objects, or strings. Strings are immutable sequences of Unicode code points.*\n",
    "\n",
    "the vast majority of text available in the internet is encoded using Unicode. The `ord()` function returns the number representing the unicode code of a given character."
   ],
   "id": "81c9ea8f97185f1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T22:16:07.544635Z",
     "start_time": "2025-08-03T22:16:07.540383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "characs = ['ლ', 'პ', '🌞', '🔥', '༂', '༅']\n",
    "for char in characs:\n",
    "    print(f\"Character: {char} -> Unicode code: {ord(char)}\")"
   ],
   "id": "a66a4630dba6b746",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: ლ -> Unicode code: 4314\n",
      "Character: პ -> Unicode code: 4318\n",
      "Character: 🌞 -> Unicode code: 127774\n",
      "Character: 🔥 -> Unicode code: 128293\n",
      "Character: ༂ -> Unicode code: 3842\n",
      "Character: ༅ -> Unicode code: 3845\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "computers ultimately read and write bytes, and not abstract code points like the ones shown above. An *encoding* is a mapping turning each code point sequence to a byte sequence (a process called serialization) and back (deserialization). The Unicode Standard defines three main character encoding standards used for electronic communication:\n",
    "\n",
    "- UTF-8 -> 1 byte unit size, variable length (1-4 bytes per code point), typically used in the web, APIs and Unix.\n",
    "- UTF-16 -> 2 bytes unit size (variable length).\n",
    "- UTF-32 -> 4 bytes unit size (fixed-width)\n",
    "\n",
    "almost every single webpage is transmitted as UTF-8, mostly due to the following reasons:\n",
    "- it is the only encoding standard that is backward compatible with ASCII (another encoding standard used in a lot of legacy tooling) meaning that any text file encoded in ASCII can be decoded as UTF-8 to get exactly the same result.\n",
    "- it is space efficient, at least for latin corpora. English text, for instance, stays at around 1 byte per character, whereas UTF-16 or 32 double or quadruple it (see below)."
   ],
   "id": "9e4ef38b1549ed28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T22:16:07.583335Z",
     "start_time": "2025-08-03T22:16:07.576850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "samples = {\n",
    "    \"ascii\": \"Hello, world!\",\n",
    "    \"mutlilang\": \"こんにちは世界🌍\", # hello world in japanese + emoji\n",
    "    \"emojis\": \"🔥🌞💧🌱\" # emojis\n",
    "}\n",
    "\n",
    "print(\"{name:10} | utf‑8 | utf‑16‑le | utf‑32‑le | Python str (CPython 3.12) sizeof\")\n",
    "print(\"---------\")\n",
    "for name, s in samples.items():\n",
    "    utf8 = len(s.encode(\"utf-8\"))\n",
    "    u16 = len(s.encode(\"utf-16-le\"))\n",
    "    u32 = len(s.encode(\"utf-32-le\"))\n",
    "    pyobj = getsizeof(s)\n",
    "    print(f\"{name:10} | {utf8:5d} | {u16:10d} | {u32:10d} | {pyobj:27d}\")"
   ],
   "id": "f21f2cf90b87d199",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{name:10} | utf‑8 | utf‑16‑le | utf‑32‑le | Python str (CPython 3.12) sizeof\n",
      "---------\n",
      "ascii      |    13 |         26 |         52 |                          62\n",
      "mutlilang  |    25 |         18 |         32 |                         108\n",
      "emojis     |    16 |         16 |         16 |                          92\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice that, for ASCII-heavy text (latin-based corpora), UTF-8 is 2x-4x smaller than UTF-18/32. let's show the proportion of zero-valued bytes and preview the first 32 bytes of each encoding:",
   "id": "854bf1c46c52b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T22:16:07.629729Z",
     "start_time": "2025-08-03T22:16:07.621990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import textwrap\n",
    "\n",
    "def hex_preview(blob:bytes, limit=32):\n",
    "    head = blob[:limit]\n",
    "    return \" \".join(f\"{b:02x}\" for b in head) + (\" ...\" if len(blob) > limit else \"\")\n",
    "\n",
    "for name, s in samples.items():\n",
    "    print(name.upper())\n",
    "    for enc in (\"utf-8\", \"utf-16-le\", \"utf-32-le\"):\n",
    "        blob = s.encode(enc)\n",
    "        zeros = blob.count(0)\n",
    "        print(f\"{enc:10} | {len(blob):2d} bytes | {zeros:2d} zero bytes | {100*zeros/len(blob):5.1f}% zeros\")\n",
    "        print(\"  \", hex_preview(blob))\n",
    "    print()\n",
    "\n"
   ],
   "id": "4a0d5ee82c2840ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCII\n",
      "utf-8      | 13 bytes |  0 zero bytes |   0.0% zeros\n",
      "   48 65 6c 6c 6f 2c 20 77 6f 72 6c 64 21\n",
      "utf-16-le  | 26 bytes | 13 zero bytes |  50.0% zeros\n",
      "   48 00 65 00 6c 00 6c 00 6f 00 2c 00 20 00 77 00 6f 00 72 00 6c 00 64 00 21 00\n",
      "utf-32-le  | 52 bytes | 39 zero bytes |  75.0% zeros\n",
      "   48 00 00 00 65 00 00 00 6c 00 00 00 6c 00 00 00 6f 00 00 00 2c 00 00 00 20 00 00 00 77 00 00 00 ...\n",
      "\n",
      "MUTLILANG\n",
      "utf-8      | 25 bytes |  0 zero bytes |   0.0% zeros\n",
      "   e3 81 93 e3 82 93 e3 81 ab e3 81 a1 e3 81 af e4 b8 96 e7 95 8c f0 9f 8c 8d\n",
      "utf-16-le  | 18 bytes |  0 zero bytes |   0.0% zeros\n",
      "   53 30 93 30 6b 30 61 30 6f 30 16 4e 4c 75 3c d8 0d df\n",
      "utf-32-le  | 32 bytes | 15 zero bytes |  46.9% zeros\n",
      "   53 30 00 00 93 30 00 00 6b 30 00 00 61 30 00 00 6f 30 00 00 16 4e 00 00 4c 75 00 00 0d f3 01 00\n",
      "\n",
      "EMOJIS\n",
      "utf-8      | 16 bytes |  0 zero bytes |   0.0% zeros\n",
      "   f0 9f 94 a5 f0 9f 8c 9e f0 9f 92 a7 f0 9f 8c b1\n",
      "utf-16-le  | 16 bytes |  0 zero bytes |   0.0% zeros\n",
      "   3d d8 25 dd 3c d8 1e df 3d d8 a7 dc 3c d8 31 df\n",
      "utf-32-le  | 16 bytes |  4 zero bytes |  25.0% zeros\n",
      "   25 f5 01 00 1e f3 01 00 a7 f4 01 00 31 f3 01 00\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "note that each ASCII character in UTF-16-LE introduces one zero byte of padding , while UTF-32-LE introduces three. that is 50% and 75% 'wasted' space respectively, for corporas at least mostly dominated by latin symbols. even more, for emoji-heavy text (which already needs more than 3 bytes in UTF-8), we still pay a 25% overhead moving to UTF-32.",
   "id": "f64c7596da8836a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
