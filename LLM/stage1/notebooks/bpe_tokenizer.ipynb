{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## tokenizing with byte-pair encoding\n",
    "\n",
    "### *Neural Machine Translation of Rare Words with Subword Units, Sennrich et. al (2015)*\n",
    "\n",
    "[Byte-pair encoding](https://arxiv.org/pdf/1508.07909) (BPE) is a compression algorithm that was first introduced by Rico Sennrich, Barry Haddow and Alexandra Birch to optimize neural machine translation, and namely to solve the problem of translation of rare words. word-level models, relying on complete words, can't possibly generate or translate words they had never seen before, a problem that becomes bigger when dealing with various dialects, alphabets, etc.\n",
    "\n",
    "Sennrich et.al showed that translation of rare words is possible through their encoding via subword units, and additionally created a vocabulary, (a set of tokens, each one of variable-length) that was fixed in size but that could handle any input given to it. the argument was that, the translation of some word is possible and 'transparent' for any competent translator, even if the word is novel or unknown, based on an analysis of known subwords contained within that word (morphemes or phonemes).\n",
    "\n",
    "> a very simplified view of the algorithm:\n",
    ">\n",
    "> - Start with a vocabulary that is just the raw symbols (characters)\n",
    "> - Repeatedly merge the most-common adjacent symbol pairs into new, longer tokens.\n",
    "> - Stop when you hit a fixed vocab size (e.g. 50k)\n",
    "\n",
    "### *Language Models are Unsupervised Multitask Learners, Radford et.al (2019)*\n",
    "\n",
    "in the context of language models, this paper introduced BPE as a mechanism for language models, recognizing and leveraging the many desirable properties that the algorithm had in the context of language modeling. alternatives to BPE included classic word-level tokenisers that, as Sennrich et.al suggested, choken on funky slang, creative spelling, emojis, and anything else they had never seen.\n",
    "\n",
    "> nice compression (one token per known word) but brittle. any typoe or neologism explodes into an `<unk>` or a pile of fallback characters.\n",
    "\n",
    "character-level models fixed this coverage, but exploded sequence length and more easily lost track of higher-level patterns.\n",
    "\n",
    "> antidisestablishmentarianism -> 28 tokens. long sequences, slower training, limits how much context we can put into the model's window.\n",
    "\n",
    "BPE is described to be a middle ground between both: shorter than characters, nimbler than words. It produces fewer tokens than a pure character model (shorter sequences) while staying more adaptable than a pure word model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9cbb49e2e80966f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### implementing byte-pair encoding\n",
    "\n",
    "Strings are sequences of Unicode code points. [Unicode](https://en.wikipedia.org/wiki/Unicode) is a character encoding standard defining more than 150,000 characters and 168 scripts.\n",
    "\n",
    "> From the Python documentation: *Textual data in Python is handled with str objects, or strings. Strings are immutable sequences of Unicode code points.*\n",
    "\n",
    "the vast majority of text available in the internet is encoded using Unicode. The `ord()` function returns the number representing the unicode code of a given character."
   ],
   "id": "81c9ea8f97185f1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.013141Z",
     "start_time": "2025-08-04T10:38:35.009398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "characs = ['áƒš', 'áƒž', 'ðŸŒž', 'ðŸ”¥', 'à¼‚', 'à¼…']\n",
    "for char in characs:\n",
    "    print(f\"Character: {char} -> Unicode code: {ord(char)}\")"
   ],
   "id": "a66a4630dba6b746",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: áƒš -> Unicode code: 4314\n",
      "Character: áƒž -> Unicode code: 4318\n",
      "Character: ðŸŒž -> Unicode code: 127774\n",
      "Character: ðŸ”¥ -> Unicode code: 128293\n",
      "Character: à¼‚ -> Unicode code: 3842\n",
      "Character: à¼… -> Unicode code: 3845\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "computers ultimately read and write bytes, and not abstract code points like the ones shown above. An *encoding* is a mapping turning each code point sequence to a byte sequence (a process called serialization) and back (deserialization). The Unicode Standard defines three main character encoding standards used for electronic communication:\n",
    "\n",
    "- UTF-8 -> 1 byte unit size, variable length (1-4 bytes per code point), typically used in the web, APIs and Unix.\n",
    "- UTF-16 -> 2 bytes unit size (variable length).\n",
    "- UTF-32 -> 4 bytes unit size (fixed-width)\n",
    "\n",
    "almost every single webpage is transmitted as UTF-8, mostly due to the following reasons:\n",
    "- it is the only encoding standard that is backward compatible with ASCII (another encoding standard used in a lot of legacy tooling) meaning that any text file encoded in ASCII can be decoded as UTF-8 to get exactly the same result.\n",
    "- it is space efficient, at least for latin corpora. English text, for instance, stays at around 1 byte per character, whereas UTF-16 or 32 double or quadruple it (see below).\n",
    "\n",
    "> a note on something i was personally confused with:\n",
    ">\n",
    "> - UTF-8 is 'variable-length' because different characters need 1-4 bytes to encode. but **each individual byte** can still store only 0-255 ($2^8 - 1$), being those 256 byte values what a byte-level tokenizer treats as its entire vocabulary. Do not confuse symbols with byte tokens. UTF-8 has 256 distinct byte tokens, which you can combine to do way more than 256 characters."
   ],
   "id": "9e4ef38b1549ed28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.047259Z",
     "start_time": "2025-08-04T10:38:35.041586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "samples = {\n",
    "    \"ascii\": \"Hello, world!\",\n",
    "    \"mutlilang\": \"ã“ã‚“ã«ã¡ã¯ä¸–ç•ŒðŸŒ\", # hello world in japanese + emoji\n",
    "    \"emojis\": \"ðŸ”¥ðŸŒžðŸ’§ðŸŒ±\" # emojis\n",
    "}\n",
    "\n",
    "print(\"{name:10} | utfâ€‘8 | utfâ€‘16â€‘le | utfâ€‘32â€‘le | Python str (CPython 3.12) sizeof\")\n",
    "print(\"---------\")\n",
    "for name, s in samples.items():\n",
    "    utf8 = len(s.encode(\"utf-8\"))\n",
    "    u16 = len(s.encode(\"utf-16-le\"))\n",
    "    u32 = len(s.encode(\"utf-32-le\"))\n",
    "    pyobj = getsizeof(s)\n",
    "    print(f\"{name:10} | {utf8:5d} | {u16:10d} | {u32:10d} | {pyobj:27d}\")"
   ],
   "id": "f21f2cf90b87d199",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{name:10} | utfâ€‘8 | utfâ€‘16â€‘le | utfâ€‘32â€‘le | Python str (CPython 3.12) sizeof\n",
      "---------\n",
      "ascii      |    13 |         26 |         52 |                          62\n",
      "mutlilang  |    25 |         18 |         32 |                         108\n",
      "emojis     |    16 |         16 |         16 |                          92\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice that, for ASCII-heavy text (latin-based corpora), UTF-8 is 2x-4x smaller than UTF-18/32. let's show the proportion of zero-valued bytes and preview the first 32 bytes of each encoding:",
   "id": "854bf1c46c52b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.080535Z",
     "start_time": "2025-08-04T10:38:35.074255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hex_preview(blob:bytes, limit=32):\n",
    "    head = blob[:limit]\n",
    "    return \" \".join(f\"{b:02x}\" for b in head) + (\" ...\" if len(blob) > limit else \"\")\n",
    "\n",
    "for name, s in samples.items():\n",
    "    print(name.upper())\n",
    "    for enc in (\"utf-8\", \"utf-16-le\", \"utf-32-le\"):\n",
    "        blob = s.encode(enc)\n",
    "        zeros = blob.count(0)\n",
    "        print(f\"{enc:10} | {len(blob):2d} bytes | {zeros:2d} zero bytes | {100*zeros/len(blob):5.1f}% zeros\")\n",
    "        print(\"  \", hex_preview(blob))\n",
    "    print()\n",
    "\n"
   ],
   "id": "4a0d5ee82c2840ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCII\n",
      "utf-8      | 13 bytes |  0 zero bytes |   0.0% zeros\n",
      "   48 65 6c 6c 6f 2c 20 77 6f 72 6c 64 21\n",
      "utf-16-le  | 26 bytes | 13 zero bytes |  50.0% zeros\n",
      "   48 00 65 00 6c 00 6c 00 6f 00 2c 00 20 00 77 00 6f 00 72 00 6c 00 64 00 21 00\n",
      "utf-32-le  | 52 bytes | 39 zero bytes |  75.0% zeros\n",
      "   48 00 00 00 65 00 00 00 6c 00 00 00 6c 00 00 00 6f 00 00 00 2c 00 00 00 20 00 00 00 77 00 00 00 ...\n",
      "\n",
      "MUTLILANG\n",
      "utf-8      | 25 bytes |  0 zero bytes |   0.0% zeros\n",
      "   e3 81 93 e3 82 93 e3 81 ab e3 81 a1 e3 81 af e4 b8 96 e7 95 8c f0 9f 8c 8d\n",
      "utf-16-le  | 18 bytes |  0 zero bytes |   0.0% zeros\n",
      "   53 30 93 30 6b 30 61 30 6f 30 16 4e 4c 75 3c d8 0d df\n",
      "utf-32-le  | 32 bytes | 15 zero bytes |  46.9% zeros\n",
      "   53 30 00 00 93 30 00 00 6b 30 00 00 61 30 00 00 6f 30 00 00 16 4e 00 00 4c 75 00 00 0d f3 01 00\n",
      "\n",
      "EMOJIS\n",
      "utf-8      | 16 bytes |  0 zero bytes |   0.0% zeros\n",
      "   f0 9f 94 a5 f0 9f 8c 9e f0 9f 92 a7 f0 9f 8c b1\n",
      "utf-16-le  | 16 bytes |  0 zero bytes |   0.0% zeros\n",
      "   3d d8 25 dd 3c d8 1e df 3d d8 a7 dc 3c d8 31 df\n",
      "utf-32-le  | 16 bytes |  4 zero bytes |  25.0% zeros\n",
      "   25 f5 01 00 1e f3 01 00 a7 f4 01 00 31 f3 01 00\n",
      "\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "note that each ASCII character in UTF-16-LE introduces one zero byte of padding , while UTF-32-LE introduces three. that is 50% and 75% 'wasted' space respectively, for corporas at least mostly dominated by latin symbols. even more, for emoji-heavy text (which already needs more than 3 bytes in UTF-8), we still pay a 25% overhead moving to UTF-32.\n",
    "\n",
    "### using raw utf-8 bytes as a tokens looks tempting...\n",
    "\n",
    "Using raw UTF-8 bytes as tokens is an option. at the end of the day, it provides us with a tiny embedding table (only 256 possible bytes) while still overcoming the out-of-vocabulary risk. additionally, encoding/decoding any given text with it is trivial, and already implemented for us:"
   ],
   "id": "f64c7596da8836a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.119496Z",
     "start_time": "2025-08-04T10:38:35.113574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"to be or not to be\"\n",
    "utf_8_text_bytes = b\"to be or not to be\"\n",
    "print(f\"Byte literal: {utf_8_text_bytes}\")\n",
    "print(f\"Class of byte literal: {type(utf_8_text_bytes)}\")\n",
    "print(f\"Elements of byte literal:\\n {list(utf_8_text_bytes)}\")\n",
    "og_text = utf_8_text_bytes.decode(\"utf-8\")\n",
    "print(f\"Original text: {og_text}\")\n",
    "print(f\"Class of original text: {type(og_text)}\")\n",
    "print(f\"Elements of original text:\\n {list(og_text)}\")"
   ],
   "id": "5e9d6412de1a9fec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte literal: b'to be or not to be'\n",
      "Class of byte literal: <class 'bytes'>\n",
      "Elements of byte literal:\n",
      " [116, 111, 32, 98, 101, 32, 111, 114, 32, 110, 111, 116, 32, 116, 111, 32, 98, 101]\n",
      "Original text: to be or not to be\n",
      "Class of original text: <class 'str'>\n",
      "Elements of original text:\n",
      " ['t', 'o', ' ', 'b', 'e', ' ', 'o', 'r', ' ', 'n', 'o', 't', ' ', 't', 'o', ' ', 'b', 'e']\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This would then be a byte-level tokenizer, simply breaking the UTF-8 stream at every individual byte boundary. higher-plane characters stretching over four bytes just means four consecutive byte-tokens--it does not change the size of the vocab itself. as the above introduced papers explain, this would introduce three problems:\n",
    "\n",
    "- **longer sequences**: to represent any given character we need at least 1 byte (every non-latin or non-ASCII code point expanding to 2-4 bytes). this would force us towards very long sequences of tokens. the complexity of transformers, the inner mechanism of a large language model, scales by $L^2$ in sequence length $L$. the longer the inputs, the more attention work, and the more reduced, in terms of symbols, our context window would be.\n",
    "- **tokens do not line up with meaning**: a single emoji, such as âš½ï¸, lands in the model as 2-4 unrelated integers. since we want the network to first learn how bytes compose into code points (or into words and longer grammatical structures) before it can learn anything about semantics, this is a bit of wasted capacity and training time."
   ],
   "id": "f213a7d06b04fdbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.160564Z",
     "start_time": "2025-08-04T10:38:35.155003Z"
    }
   },
   "cell_type": "code",
   "source": "list(\"âš½\".encode(\"utf-8\"))",
   "id": "858b0caaef092957",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[226, 154, 189]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- **the very slim vocab hurts compression**: byte-level needs length of word in bytes tokens. while the size of the embedding table shrinks, the computation costs in terms of later activations, positional embeddings, and attention operations grow far faster. in the example below, for instance, a text in japanese explodes from 8 logical characters to 25 byte-tokens, and emojis explode even more.",
   "id": "d1071f7527d56dbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.204161Z",
     "start_time": "2025-08-04T10:38:35.197675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def describe(name, s):\n",
    "    b = s.encode(\"utf-8\")\n",
    "    b_tokens = list(b)\n",
    "    cp_tokens = list(s)\n",
    "\n",
    "    print(f\"{name:10} | bytes: {len(b_tokens):2d} tokens | code-points: {len(cp_tokens):2d} tokens\")\n",
    "    print(f\" unique byte-tokens used: {len(set(b_tokens)):3d} / 256\")\n",
    "    print(f\"preview bytes: {b_tokens[:16]}\")\n",
    "    print()\n",
    "\n",
    "for n, s in samples.items():\n",
    "    describe(n, s)"
   ],
   "id": "f739fa3fd591e0d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii      | bytes: 13 tokens | code-points: 13 tokens\n",
      " unique byte-tokens used:  10 / 256\n",
      "preview bytes: [72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33]\n",
      "\n",
      "mutlilang  | bytes: 25 tokens | code-points:  8 tokens\n",
      " unique byte-tokens used:  16 / 256\n",
      "preview bytes: [227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 228]\n",
      "\n",
      "emojis     | bytes: 16 tokens | code-points:  4 tokens\n",
      " unique byte-tokens used:   9 / 256\n",
      "preview bytes: [240, 159, 148, 165, 240, 159, 140, 158, 240, 159, 146, 167, 240, 159, 140, 177]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "let's see a very basic approximation of the additional compute associated with this explosion:",
   "id": "a3576420a7340718"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.262465Z",
     "start_time": "2025-08-04T10:38:35.256969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transformer_flops(n_layers, dim, seq_len, factor=2):\n",
    "    return n_layers * factor * (seq_len ** 2) * dim\n",
    "\n",
    "L  = 12           # layers\n",
    "H  = 768          # hidden size\n",
    "char_len = 8 # japanese text numbers\n",
    "byte_len = 25\n",
    "\n",
    "print(\"Char-level  :\", f\"{transformer_flops(L, H, char_len):,.2f} floating point operations needed.\")\n",
    "print(\"Byte-level  :\", f\"{transformer_flops(L, H, byte_len):,.2f} floating point operations needed.\")\n"
   ],
   "id": "a72ba75486153942",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char-level  : 1,179,648.00 floating point operations needed.\n",
      "Byte-level  : 11,520,000.00 floating point operations needed.\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### utf-8 aware $\\ne$ byte-bound\n",
    "\n",
    "We want to start and keep UTF-8 as a base because it gives us three very important guarantees: most web-based text and any future Unicode character already has a UTF-8 spelling (our tokenizer would never go out of date, unless the Unicode standard is replaced), we have a method for loss-free round-trips (text -> tokens -> text is always possible and exact) and UTF-8 can be processed left-to-right without backtracking, good for data pipelines, distributed and sharded datasets.\n",
    "\n",
    "therfore, using the same 256-byte alphabet, we want to apply some sort of clever operations on top that shrinks average sequence length, caps vocab at a manageable size, remains fully reversible and stays future-proof."
   ],
   "id": "1c015e412b7ca1d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### actually implementing byte-pair encoding\n",
    "\n",
    "let's use morse code to see Byte-Pair encoding in action. the morse code alphabet is composed only of two symbols, composed each of a single byte."
   ],
   "id": "aa5450c71fc37a64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.300082Z",
     "start_time": "2025-08-04T10:38:35.295003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "morse_alphabet = [\".\", \"-\"]\n",
    "for token in morse_alphabet:\n",
    "    print(f\"Byte-encodings of morse code ({token}): {list(token.encode('utf-8'))}\")"
   ],
   "id": "b9a640e93cb32e25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte-encodings of morse code (.): [46]\n",
      "Byte-encodings of morse code (-): [45]\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "suppose for instance we wanted to encode ['hello hello' in Morse](https://morsecode.world/international/translator.html):\n",
    "\n",
    "``` bash\n",
    ".... . .-.. .-.. --- .... . .-.. .-.. ---\n",
    "\n",
    "# hello hello in morse\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "6e857f52399c4ff0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.338510Z",
     "start_time": "2025-08-04T10:38:35.333090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hello_hello = \".... . .-.. .-.. --- .... . .-.. .-.. ---\"\n",
    "\n",
    "# encode to raw bytes\n",
    "byte_tokens = list(hello_hello.encode(\"utf-8\"))\n",
    "unique_byte_tokens = set(byte_tokens)\n",
    "\n",
    "# map each byte back to its printable char\n",
    "unique_chars = [bytes([b]).decode(\"utf-8\") for b in unique_byte_tokens]\n",
    "\n",
    "\n",
    "print(f\"Raw byte list:\\n{byte_tokens}\\n\")\n",
    "print(f\"Sequence length: {len(byte_tokens)}\")\n",
    "print(f\"Unique byte tokens: {len(unique_byte_tokens)} â†’ {unique_byte_tokens}\")\n",
    "print(f\"Unique char tokens: {len(unique_chars)}   â†’ {unique_chars}\")"
   ],
   "id": "7b52c10136ad32d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw byte list:\n",
      "[46, 46, 46, 46, 32, 46, 32, 46, 45, 46, 46, 32, 46, 45, 46, 46, 32, 45, 45, 45, 32, 46, 46, 46, 46, 32, 46, 32, 46, 45, 46, 46, 32, 46, 45, 46, 46, 32, 45, 45, 45]\n",
      "\n",
      "Sequence length: 41\n",
      "Unique byte tokens: 3 â†’ {32, 45, 46}\n",
      "Unique char tokens: 3   â†’ [' ', '-', '.']\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "so the Morse-encodeed 'hello hello' uses three distincy byte tokens only. that is a very small vocab: the embedding table size is very small, but sequence length explodes (41 symbols for just two words). this is exactly the trade-off we described earlier.\n",
    "\n",
    "to fix this, the BPE iteratively finds the most common adjacent pair of characters, fuses them into a single, new glyph and replaces every occurrence. here we find the most common adjacent pairs of characters:"
   ],
   "id": "2a2fa5584848c656"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.372715Z",
     "start_time": "2025-08-04T10:38:35.367797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(zip(hello_hello, hello_hello[1:]))\n",
    "print(\"Counts of adjacent pairs:\")\n",
    "for c in counts:\n",
    "    print(f\"\\n{c}, {counts[c]}\")\n",
    "    ((a, b)), freq = counts.most_common(1)[0]\n",
    "print(\"Most common pair:\")\n",
    "print(f\"'{a}{b}' with freq: {freq}\") if freq > 1 else None  # stop when every pair is unique"
   ],
   "id": "a9018a89b27a6ca9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of adjacent pairs:\n",
      "\n",
      "('.', '.'), 10\n",
      "\n",
      "('.', ' '), 8\n",
      "\n",
      "(' ', '.'), 7\n",
      "\n",
      "('.', '-'), 4\n",
      "\n",
      "('-', '.'), 4\n",
      "\n",
      "(' ', '-'), 2\n",
      "\n",
      "('-', '-'), 4\n",
      "\n",
      "('-', ' '), 1\n",
      "Most common pair:\n",
      "'..' with freq: 10\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.416203Z",
     "start_time": "2025-08-04T10:38:35.409891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# helper function\n",
    "def most_common_pair(seq):\n",
    "    counts = Counter(zip(seq, seq[1:]))\n",
    "    ((a, b)), freq = counts.most_common(1)[0]\n",
    "    return (a, b, freq) if freq > 1 else None  # stop when every pair is unique\n",
    "\n",
    "most_common_pair(list(hello_hello))"
   ],
   "id": "57f9201682f66142",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', '.', 10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "then, we mint a new glyph using the most common pair, and we replace every occurrence of it. For example, we mint a new symbol called 'A', and 'A' replaces every '..'. For instance, the starting symbols `....` become `AA`.",
   "id": "3dbe29378ce184d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.449257Z",
     "start_time": "2025-08-04T10:38:35.442673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fuse_once(seq, pair, new_sym):\n",
    "    a, b = pair\n",
    "    out, i = [], 0\n",
    "    while i < len(seq):\n",
    "        if i + 1 < len(seq) and seq[i]==a and seq[i+1]==b:\n",
    "            out.append(new_sym); i += 2\n",
    "        else:\n",
    "            out.append(seq[i]);  i += 1\n",
    "    return out\n",
    "\n",
    "alphabet = {'.', '-'}\n",
    "rules = []\n",
    "\n",
    "pair_found = most_common_pair(list(hello_hello))\n",
    "a, b, freq = pair_found\n",
    "rules.append(f\"A = {a}{b}\")\n",
    "alphabet.add('A')\n",
    "\n",
    "new_hello_hello = fuse_once(hello_hello, (a,b), \"A\")\n",
    "print(f\"New String: {new_hello_hello}\")\n",
    "print(f\"Rule: {rules}\")"
   ],
   "id": "de27bc221114866a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New String: ['A', 'A', ' ', '.', ' ', '.', '-', 'A', ' ', '.', '-', 'A', ' ', '-', '-', '-', ' ', 'A', 'A', ' ', '.', ' ', '.', '-', 'A', ' ', '.', '-', 'A', ' ', '-', '-', '-']\n",
      "Rule: ['A = ..']\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "the symbol `A` then becomes part of the vocabulary, and the same process is run iteratively until we have hit a predetermine number of tokens, or alternatively until the remaining bytes have adjacent pairs that do not occur more than once. the next iteration then would look like this.",
   "id": "416ea0477d8aa03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.548831Z",
     "start_time": "2025-08-04T10:38:35.543764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pair_found = most_common_pair(list(new_hello_hello))\n",
    "a, b, freq = pair_found\n",
    "rules.append(f\"B = {a}{b}\")\n",
    "alphabet.add('B')\n",
    "\n",
    "new_hello_hello = fuse_once(new_hello_hello, (a,b), \"B\")\n",
    "print(f\"New String: {new_hello_hello}\")\n",
    "print(f\"Rule: {rules}\")"
   ],
   "id": "79733f645b6c02db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New String: ['A', 'B', '.', ' ', '.', '-', 'B', '.', '-', 'B', '-', '-', '-', ' ', 'A', 'B', '.', ' ', '.', '-', 'B', '.', '-', 'B', '-', '-', '-']\n",
      "Rule: ['A = ..', 'B = A ']\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "so the sequence `A `, (an A followed by a space, or `.. `) gets replaced by a new symbol `B` which is then added to our vocabulary. running this algorithm iteratively until the end would result in something like the following:",
   "id": "2a07ca7f53b28867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:38:35.698553Z",
     "start_time": "2025-08-04T10:38:35.690305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "\n",
    "alphabet = {'.', '-'}\n",
    "rules    = []\n",
    "fresh    = iter(string.ascii_uppercase)\n",
    "step     = 1\n",
    "\n",
    "print(\"Start :\", \"\".join(hello_hello))\n",
    "while True:\n",
    "    found = most_common_pair(hello_hello)\n",
    "    if not found:\n",
    "        break\n",
    "    a, b, freq = found\n",
    "    new_sym    = next(fresh)          # mint A, B, C, ...\n",
    "    rules.append(f\"{new_sym} = {a}{b}\")\n",
    "    alphabet.add(new_sym)\n",
    "    hello_hello = fuse_once(hello_hello, (a, b), new_sym)\n",
    "\n",
    "    print(f\"Step {step:>2} : replace '{a}{b}' â†’ '{new_sym}' \"\n",
    "          f\"(appeared {freq}Ã—)  â†’  {''.join(hello_hello)}\")\n",
    "    step += 1\n",
    "\n",
    "print(\"\\nCompressed sequence :\", \"\".join(hello_hello))\n",
    "print(\"Merge rules (oldest â†’ newest):\")\n",
    "for r in rules:\n",
    "    print(\" \", r)"
   ],
   "id": "987fee7b63e80d93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : .... . .-.. .-.. --- .... . .-.. .-.. ---\n",
      "Step  1 : replace '..' â†’ 'A' (appeared 10Ã—)  â†’  AA . .-A .-A --- AA . .-A .-A ---\n",
      "Step  2 : replace 'A ' â†’ 'B' (appeared 6Ã—)  â†’  AB. .-B.-B--- AB. .-B.-B---\n",
      "Step  3 : replace 'B.' â†’ 'C' (appeared 4Ã—)  â†’  AC .-C-B--- AC .-C-B---\n",
      "Step  4 : replace '--' â†’ 'D' (appeared 4Ã—)  â†’  AC .-C-BD- AC .-C-BD-\n",
      "Step  5 : replace 'AC' â†’ 'E' (appeared 2Ã—)  â†’  E .-C-BD- E .-C-BD-\n",
      "Step  6 : replace 'E ' â†’ 'F' (appeared 2Ã—)  â†’  F.-C-BD- F.-C-BD-\n",
      "Step  7 : replace 'F.' â†’ 'G' (appeared 2Ã—)  â†’  G-C-BD- G-C-BD-\n",
      "Step  8 : replace 'G-' â†’ 'H' (appeared 2Ã—)  â†’  HC-BD- HC-BD-\n",
      "Step  9 : replace 'HC' â†’ 'I' (appeared 2Ã—)  â†’  I-BD- I-BD-\n",
      "Step 10 : replace 'I-' â†’ 'J' (appeared 2Ã—)  â†’  JBD- JBD-\n",
      "Step 11 : replace 'JB' â†’ 'K' (appeared 2Ã—)  â†’  KD- KD-\n",
      "Step 12 : replace 'KD' â†’ 'L' (appeared 2Ã—)  â†’  L- L-\n",
      "Step 13 : replace 'L-' â†’ 'M' (appeared 2Ã—)  â†’  M M\n",
      "\n",
      "Compressed sequence : M M\n",
      "Merge rules (oldest â†’ newest):\n",
      "  A = ..\n",
      "  B = A \n",
      "  C = B.\n",
      "  D = --\n",
      "  E = AC\n",
      "  F = E \n",
      "  G = F.\n",
      "  H = G-\n",
      "  I = HC\n",
      "  J = I-\n",
      "  K = JB\n",
      "  L = KD\n",
      "  M = L-\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "to be clear, the vocabulary keeps every symbol that was minted along the way. The final sequence being `M M` reflects (apart from the original string being a repetition of the same structure with a space in between, namely 'hello hello') that the final sequence shrank to two tokens (`M`, `space`). The embedding table still has the full amount of original and generated entries in the vocabulary:",
   "id": "ef94744622402851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T10:48:03.015724Z",
     "start_time": "2025-08-04T10:48:03.011328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Final compressed sequence:\", ''.join(hello_hello))\n",
    "print(\"Vocabulary size:\", len(alphabet))\n",
    "print(\"Vocabulary symbols:\", sorted(alphabet))\n"
   ],
   "id": "5a18cf4f950eb2da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final compressed sequence: M M\n",
      "Vocabulary size: 15\n",
      "Vocabulary symbols: ['-', '.', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M']\n"
     ]
    }
   ],
   "execution_count": 126
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
