{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Attention Mechanisms\n",
    "In this notebook, we will sequentially implement different variants of attention mechanisms. These variants will build on each other, with the goal of finally creating a compact, efficient implementation of an attention mechanism, which we can then plug into our LLM architecture.\n",
    "\n",
    "**Simple Self-Attention**: Introduce the broader idea behind attention.\n",
    "\n",
    "**Self-Attention**: Trainable weights that forms the basis of the mechanisms used in LLMs.\n",
    "\n",
    "**Causal Attention**: A self-attention variant allowing a model to consider only previous and current inputs in a sequence, ensuring temporal order during text generation.\n",
    "\n",
    "**Multi-head Attention**: A self-attention and causal attension extension, which enables the model to simultaneously attend to information from different representation subspaces.\n",
    "\n",
    "#### Why Attention?\n",
    "In machine translation, it is not possible to merely translate word by word. The translation process requires contextual understandng and grammatical alignment.\n",
    "\n",
    "- \"Kannst du mir helfen diesen Satz zu uebersetzen\" should not be translated to \"Can you me help this sentence to translate\", but rather to \"Can you help me translate this sentence\".\n",
    "- Certain words require access to words appearing before or later in the original sentence. For instance, the verb \"to translate\" should be used in the context of \"this sentence\", and not independently.\n",
    "\n",
    "Typically, to overcome this challenge, deep neural networks with two submodules are used:\n",
    "\n",
    "- **encoder**: first read in and process the entire text (already done in the `preprocessing.ipynb` notebook).\n",
    "\n",
    "- **decoder**: produces the translated text.\n",
    "\n",
    "Pre-LLM architectures typically involved recurrent neural networks, a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data. In this many-to-one RNN architecture, the input text is fed token by token into the encoder, which processes it sequentially. The terminal state of the encoder is a memory cell, known as the hidden state, which encodes the entire input. This hidden state is then fed to a decoder that would then generate the translated sentence, word by word, one word at a time.\n",
    "\n",
    "- While the encoder is many-to-one, the decoder is a one-to-many architecture, since the hidden state is passed at every step of the decoding process.\n",
    "\n",
    "**encoder-decoder RNNs had many shortcomings that motivated the design of attention mechanisms**, namely that the it was not possible to access earlier hidden states from the encoder during the decoding phase, since we rely on a single hidden state containing all the relevant information. Context was lost, especially in complex sentences where dependencies span larger distances."
   ],
   "id": "8228f801c75cb45f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *Non-Modified RNN Encoders*\n",
    "\n",
    "\n"
   ],
   "id": "18b81aa0a5f2b027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T21:33:03.060365Z",
     "start_time": "2025-03-30T21:32:37.415165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout = None):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        if dropout:\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, word_inputs, hidden):\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_size))\n",
    "        return hidden"
   ],
   "id": "ad0121d21ac988c8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The class EncoderRNN above depicts how a non-modified encoder RNN works. At every step of decoding, the decoder is given an input token as well as a hidden state. The relevant building blocks are:\n",
    "\n",
    "- `torch.nn.Embedding`: As described before, it is simply a lookup table storing the embeddings of a fixed dictionary, of a fixed size. The input to this module is a list of indices, and the output is the corresponding word embeddings.\n",
    "\n",
    "    - Note that the module has learnable weights, a tensor, of the shape (num_embeddings, embedding_dim). These are traditionally initialized from N(0,1).\n",
    "\n",
    "\n",
    "- `torch.nn.GRU`: Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. More specifically, this means that, for each element in an input, the following are computed at each layer:\n",
    "$$\n",
    "\\begin{align*}\n",
    "r_t &= \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
    "z_t &= \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
    "n_t &= \\tanh(W_{in} x_t + b_{in} + r_t \\odot (W_{hn} h_{(t-1)} + b_{hn})) \\\\\n",
    "h_t &= (1 - z_t) \\odot n_t + z_t \\odot h_{(t-1)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "    - where $h_{t}$ is the hidden state at time $t$, $x$ is the input at time $t$, and $r_{t}$, $z_{t}$, and $n_{t}$ are the reset, update and new gates, respectively. $\\odot$ is the Hadamard product (or element-wise product).\n",
    "        - the reset gate determines how much of a previous hidden state $h_{(t-1)}$ to forget, while the update gate determines how much of the candidate activation vector $n_{t}$ to incorporate into the new hidden state, $h_{t}$.\n",
    "        - the input $x_{t}^{(l)}$ of the $l$-th layer ($l >= 2$) is the hidden state of $h_{t}^{(l-1)}$ multiplied by a dropout $d_{t}^{(l-1)}$, a Bernoulli random variable with probability `dropout`."
   ],
   "id": "8f3b5571ad2a0b83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *The Badhanau Attention Mechanism*\n",
    "As a result of the major shortcomings of traditional RNN encoder-decoders, researchers eventually developed the **Badhanau Attention Mechanism** for RNNs in 2014. In this modification, the decoder can selectively access different parts of the input sequence at each decoding step.\n",
    "\n",
    "- When generating an output token, the model has a way to access all input tokens.Input tokens have contain a measure of how important the input token is for the respective output tokens.\n",
    "\n",
    "- The new module computes an 'energy' score for each encoder output given the current decoder hidden state. The score is computed as $score = V_{a}^{T} tanh(W_{a} * s_{t-1} + U_{a} * h_{i})$. These scores are later normalized to produce attention weights.\n",
    "    - This module is then used by the `AttnDecoderRNN`, which embeds the input token, applies dropout, and uses the attention module to compute a context vector. The context vector and the embedded input are then concatenated, fed into a GRU, and output probabilities over the target dictionary are produced."
   ],
   "id": "a774114e8386defa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T21:41:22.455708Z",
     "start_time": "2025-03-30T21:41:22.444160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BadhanauAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BadhanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # linear layers to transform the decoder hidden state and encoder outputs.\n",
    "        self.Wa = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Ua = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        # to produce scalar score, a parameter vector\n",
    "        self.Va = torch.nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        decoder_hidden: (1, batch_size, hidden_size) - current decoder hidden state.\n",
    "        encoder_outputs: (seq_len, batch_size, hidden_size) - all encoder outputs\n",
    "        \"\"\"\n",
    "        # squeeze time dimension from the decoder hidden state (batch_size, hidden_size)\n",
    "        decoder_hidden = decoder_hidden.squeeze(0)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "        # expand decoder sates to (batch, seq_len, hidden_size)\n",
    "        decoder_hidden_expanded = decoder_hidden.unsqueeze(1).expand_as(encoder_outputs)\n",
    "        nrg = torch.tanh(self.Wa(decoder_hidden_expanded) + self.Ua(encoder_outputs))\n",
    "        attention_scores = self.Va(nrg).squeeze(2)\n",
    "        # normalize the scores to probs\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        #compute context vector as weighted sum of encoder outputs:\n",
    "        # (batch_size, 1, seq_len) x (batch_size, seq_len, hidden_size) --> (batch_size, 1, hidden_size)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        context = context.squeeze(1)\n",
    "        return context, attention_weights\n",
    "\n",
    "class AttnDecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout=None):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.attention = BadhanauAttention(hidden_size)\n",
    "        #GRU now takes the concatenated [embedded; context] vector as input.\n",
    "        self.gru = torch.nn.GRU(hidden_size * 2, hidden_size)\n",
    "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        input_token: (batch,) token indices for the current decoder input.\n",
    "        hidden: (1, batch, hidden_size) current decoder hidden state.\n",
    "        encoder_outputs: (seq_len, batch, hidden_size) outputs from the encoder.\n",
    "        \"\"\"\n",
    "        # Get embedding of current token and apply dropout\n",
    "        embedded = self.embedding(input_token).unsqueeze(0)  # shape: (1, batch, hidden_size)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # Compute the context vector using attention\n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        # Prepare context for concatenation: (1, batch, hidden_size)\n",
    "        context = context.unsqueeze(0)\n",
    "        # Concatenate embedded input and context vector\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)  # shape: (1, batch, 2*hidden_size)\n",
    "        # Pass through the GRU\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "        # Prepare output for final linear layer\n",
    "        output = output.squeeze(0)  # shape: (batch, hidden_size)\n",
    "        output = self.out(output)   # shape: (batch, output_size)\n",
    "        # Return log probabilities and attention weights\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights"
   ],
   "id": "9f2585d9d165c014",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
