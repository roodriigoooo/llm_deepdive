{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Attention Mechanisms\n",
    "In this notebook, we will sequentially implement different variants of attention mechanisms. These variants will build on each other, with the goal of finally creating a compact, efficient implementation of an attention mechanism, which we can then plug into our LLM architecture.\n",
    "\n",
    "**Simple Self-Attention**: Introduce the broader idea behind attention.\n",
    "\n",
    "**Self-Attention**: Trainable weights that forms the basis of the mechanisms used in LLMs.\n",
    "\n",
    "**Causal Attention**: A self-attention variant allowing a model to consider only previous and current inputs in a sequence, ensuring temporal order during text generation.\n",
    "\n",
    "**Multi-head Attention**: A self-attention and causal attention extension, which enables the model to simultaneously attend to information from different representation subspaces.\n",
    "\n",
    "#### Why Attention?\n",
    "In machine translation, it is not possible to merely translate word by word. The translation process requires contextual understanding and grammatical alignment.\n",
    "\n",
    "- \"Kannst du mir helfen diesen Satz zu uebersetzen\" should not be translated to \"Can you me help this sentence to translate\", but rather to \"Can you help me translate this sentence\".\n",
    "- Certain words require access to words appearing before or later in the original sentence. For instance, the verb \"to translate\" should be used in the context of \"this sentence\", and not independently.\n",
    "\n",
    "Typically, to overcome this challenge, deep neural networks with two submodules are used:\n",
    "\n",
    "- **encoder**: first read in and process the entire text (already done in the `preprocessing.ipynb` notebook).\n",
    "\n",
    "- **decoder**: produces the translated text.\n",
    "\n",
    "Pre-LLM architectures typically involved recurrent neural networks, a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data. In this many-to-one RNN architecture, the input text is fed token by token into the encoder, which processes it sequentially. The terminal state of the encoder is a memory cell, known as the hidden state, which encodes the entire input. This hidden state is then fed to a decoder that would then generate the translated sentence, word by word, one word at a time.\n",
    "\n",
    "|     ![RNNEncoder-Decoder](images/RNNencoderdecoder.png)     |\n",
    "|:-----------------------------------------------------------:|\n",
    "| *RNN Encoder Decoder* (Dive Into Deep Learning Chapter 10.7 |\n",
    "\n",
    "- While the encoder is many-to-one, the decoder is a one-to-many architecture, since the hidden state is passed at every step of the decoding process.\n",
    "- While it is not strictly necessary to understand the inner workings of encoder-decoder RNNs to develop an LLM, the `seq2seq.ipynb` notebook in stage 1 aims to more deeply explore these architectures.\n",
    "\n",
    "**encoder-decoder RNNs had many shortcomings that motivated the design of attention mechanisms**, namely that the it was not possible to access earlier hidden states from the encoder during the decoding phase, since we rely on a single hidden state containing all the relevant information. Context was lost, especially in complex sentences where dependencies span larger distances.\n",
    "\n"
   ],
   "id": "8228f801c75cb45f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Simple Self-Attention\n",
    "\n",
    "The original *transformer* architecture includes a 'Self-Attention' mechanism inspired by the Badhanau attention mechanism mentioned in `seq2seq.ipynb`.\n",
    "\n",
    "A mechanism that uses self-attention allows each position in the input sequence (each word), to consider the importance of all other positions (all other words) in the same sequence when creating the embedding of such sequence.\n",
    "\n",
    "#### Why 'Self-Attention'\n",
    "\n",
    "The 'self' refers to the fact that the mechanism computes weights by assessing and learning the dependencies of different positions **within the same sequence**. The relationships of the various parts of the input itself are considered. This is in contrast to attention mechanisms that focus on assessing and learning the relationship of elements that are part of distinct sequences. Sequence-to-sequence models, for instance, where assessment might be done over an input sequence and a distinct output sequence is not a self-attentive mechanism.\n",
    "\n",
    "\n"
   ],
   "id": "286af1664c396c92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed67fcc89a5bca22"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
